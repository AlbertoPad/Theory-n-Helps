{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b4d551",
   "metadata": {},
   "source": [
    "# **¿Cómo crear una Red Neuronal.?** \n",
    "\n",
    "*by: Alberto Padilla Nieto*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b152c8b",
   "metadata": {},
   "source": [
    " ## Pasos para Red Neuronal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2aa41",
   "metadata": {},
   "source": [
    "1. **Definir el problema:** Es importante tener una comprensión clara del problema que se desea resolver antes de comenzar a diseñar la red neuronal.<br><br>\n",
    "2. **Recopilar y preparar los datos:** La calidad y cantidad de los datos de entrada es crucial para el éxito de una red neuronal. Es necesario seleccionar los datos adecuados y prepararlos para el entrenamiento.<br><br>\n",
    "3. **Regularización:** La regularización es una técnica utilizada para prevenir el sobreajuste en una red neuronal. Algunos ejemplos de técnicas de regularización incluyen L1 y L2 regularización, y la eliminación de dropout.<br><br>\n",
    "4. **Diseñar la arquitectura de la red:** La elección de la arquitectura adecuada es fundamental para el éxito de una red neuronal. Esto incluye la selección de la cantidad y tipo de capas, el número de neuronas por capa, la función de activación, y más.<br><br>\n",
    "5. **Seleccionar un algoritmo de entrenamiento:** La elección del algoritmo de entrenamiento correcto puede tener un gran impacto en el rendimiento de la red neuronal. Algunos ejemplos de algoritmos de entrenamiento incluyen backpropagation, gradiente descendente estocástico, y el método de propagación de errores hacia atrás.<br><br>\n",
    "6. **Entrenar y validar la red:** Una vez que se ha diseñado la red neuronal y se ha seleccionado el algoritmo de entrenamiento, se debe entrenar la red utilizando los datos de entrenamiento preparados anteriormente. Es importante validar el rendimiento de la red neuronal utilizando un conjunto de datos separado que no se haya utilizado durante el entrenamiento.<br><br>\n",
    "7. **Ajustar la red:** Después de validar la red, es posible que sea necesario ajustar la arquitectura o los parámetros de la red para mejorar el rendimiento.<br><br>\n",
    "8. **Evaluar la red:** Finalmente, es importante evaluar la red neuronal utilizando datos de prueba para determinar su rendimiento real en situaciones del mundo real.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a7c9f",
   "metadata": {},
   "source": [
    "# **Librerias** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69902354",
   "metadata": {},
   "source": [
    "**TensorFlow:** una plataforma de aprendizaje automático de código abierto desarrollada por Google que se utiliza para crear y entrenar modelos de aprendizaje profundo.<br>\n",
    "**Keras:** una biblioteca de redes neuronales de alto nivel que se ejecuta sobre TensorFlow y se utiliza para crear y entrenar modelos de aprendizaje profundo de manera rápida y eficiente.<br>\n",
    "~~~~\n",
    "                        import tensorflow as tf\n",
    "                        from tensorflow import keras\n",
    "~~~~\n",
    "**PyTorch:** una biblioteca de aprendizaje automático de código abierto desarrollada por Facebook que se utiliza para crear y entrenar modelos de aprendizaje profundo.<br>\n",
    "~~~~\n",
    "                        import torch\n",
    "~~~~\n",
    "**XGBoost:** una biblioteca de gradient boosting en árboles que se utiliza para la regresión y la clasificación de conjuntos de datos estructurados.<br>\n",
    "**LightGBM:** otra biblioteca de gradient boosting en árboles que se centra en la eficiencia y la velocidad de entrenamiento.<br>\n",
    "**CatBoost:** otra biblioteca de gradient boosting en árboles que se utiliza para la clasificación y la regresión, y que tiene como objetivo mejorar la precisión de los modelos y reducir el tiempo de entrenamiento.<br>\n",
    "~~~~\n",
    "                        import xgboost as xgb\n",
    "                        import lightgbm as lgb\n",
    "                        import catboost as cb\n",
    "~~~~\n",
    "**NLTK:** una biblioteca de procesamiento de lenguaje natural que se utiliza para trabajar con datos de texto y procesar el lenguaje natural.<br>\n",
    "**Gensim:** otra biblioteca de procesamiento de lenguaje natural que se utiliza para modelar y analizar grandes conjuntos de datos de texto, como colecciones de documentos o corpus.<br>\n",
    "~~~~\n",
    "                        import nltk\n",
    "                        import gensim\n",
    "~~~~\n",
    "**Cluster:** es un módulo de la biblioteca Scikit-Learn que proporciona herramientas para el clustering de datos, incluyendo el algoritmo de K-Means y Gaussian Mixture Models (GMM).<br>\n",
    "\n",
    "\n",
    "\n",
    "~~~~\n",
    "                        from sklearn.cluster import KMeans\n",
    "                        from sklearn.mixture import GaussianMixture\n",
    "~~~~\n",
    "\n",
    "**SVM (Support Vector Machine):** es un módulo que implementa un algoritmo para la clasificación y regresión. El módulo incluye dos variantes: SVC, que es la implementación de SVM para clasificación, y LinearSVC, que es la implementación de SVM para clasificación lineal.<br>\n",
    "\n",
    "~~~~\n",
    "                        from sklearn.svm import SVC\n",
    "                        from sklearn.svm import LinearSVC\n",
    "~~~~\n",
    "**Ensemble:** Estos modelos utilizan múltiples árboles de decisión para mejorar la precisión y generalización del modelo. incluyendo Random Forest Classifier y Gradient Boosting Classifier.<br>\n",
    "~~~~\n",
    "                        from sklearn.ensemble import RandomForestClassifier\n",
    "                        from sklearn.ensemble import GradientBoostingClassifier\n",
    "~~~~\n",
    "**Librerías Machine Learning**<br>\n",
    "- **Librerías gráficas:** Estas librerías permiten la visualización de datos y la creación de gráficos y diagramas para ayudar en el análisis de datos. Matplotlib.pyplot es una de las librerías más populares para visualización de datos en Python, Seaborn se enfoca en gráficos estadísticos más sofisticados y Pandas.plotting proporciona algunas herramientas de visualización integradas en Pandas para análisis de datos.\n",
    "~~~~\n",
    "                        import matplotlib.pyplot, \n",
    "                        import seaborn, \n",
    "                        import pandas.plotting\n",
    "~~~~\n",
    "\n",
    "- **Librerías de dataframes:** Pandas es una librería muy útil para el análisis de datos y la manipulación de datos en forma de tablas y dataframes. Numpy también se utiliza frecuentemente en combinación con Pandas para realizar operaciones matemáticas en los datos.\n",
    "~~~~\n",
    "                        import pandas\n",
    "                        import numpy  \n",
    "~~~~\n",
    "\n",
    "- **Librerías del sistema:** Estas librerías están relacionadas con la manipulación del sistema operativo y los archivos. Os proporciona funciones para trabajar con los sistemas operativos, Base64 se utiliza para la codificación y decodificación de datos, io.BytesIO proporciona un buffer de bytes para leer y escribir datos en memoria, PIL.Image permite la manipulación de imágenes, IPython.display muestra objetos en formato HTML en notebooks de Jupyter, tkinter es una biblioteca gráfica para interfaces de usuario en Python y tkinter.font proporciona funciones para trabajar con fuentes.\n",
    "~~~~\n",
    "                        import o \n",
    "                        import base6 \n",
    "                        import io.BytesI \n",
    "                        import PIL.Imag \n",
    "                        import IPython.displa \n",
    "                        import tkinte \n",
    "                        import tkinter.font  \n",
    "~~~~\n",
    "\n",
    "- **Librerías de informes:** Estas librerías se utilizan para generar informes y realizar análisis estadísticos. Statsmodels.api proporciona funciones para realizar análisis de regresión y otros modelos estadísticos, statsmodels.formula.api proporciona una interfaz más sencilla para trabajar con modelos estadísticos, Pandas_profiling genera informes automatizados sobre los datos, y statsmodels.stats.outliers_influence proporciona funciones para analizar los puntos atípicos en los datos.\n",
    "~~~~\n",
    "                        import statsmodels.api \n",
    "                        import statsmodels.formula.api \n",
    "                        import pandas_profiling \n",
    "                        import statsmodels.stats.outliers_influence\n",
    "~~~~\n",
    "\n",
    "- **Librerías para datos de entrenamiento y prueba:** sklearn.model_selection proporciona funciones para dividir los datos en conjuntos de entrenamiento y prueba para su uso en modelos de aprendizaje automático.\n",
    "~~~~\n",
    "                        import sklearn.model_selection\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8a21a",
   "metadata": {},
   "source": [
    "# **Carga y Exploración y Contextualizar de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c24cf3",
   "metadata": {},
   "source": [
    "# **Pre-procesamiento de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab54f93",
   "metadata": {},
   "source": [
    "- **Para datos numéricos:**\n",
    "> 1. Escalamiento para que todas las características estén en la misma escala.<br>\n",
    "> 2. Normalización para convertir las características en un rango de valores entre 0 y 1.<br>\n",
    "> 3. Manejo de datos faltantes, por ejemplo, eliminando las instancias con valores faltantes o imputando valores.<br>\n",
    "- **Para procesamiento de imágenes:**\n",
    "> 1. Redimensionamiento para asegurarse de que todas las imágenes tengan el mismo tamaño.<br>\n",
    "> 2. Normalización para convertir los valores de píxeles en un rango de valores entre 0 y 1.<br>\n",
    "> 3. Data augmentation para aumentar el tamaño del conjunto de datos y mejorar la generalización de la red.<br>\n",
    "- **Para procesamiento de lenguaje natural:**\n",
    "> 1. Tokenización para dividir el texto en palabras o en frases.<br>\n",
    "> 2. Eliminación de stopwords y signos de puntuación.<br>\n",
    "> 3. Creación de vocabulario para asignar un número a cada palabra en el conjunto de datos.<br>\n",
    "> 4. Padding para igualar la longitud de las secuencias.<br>\n",
    "- **Para Series temporales:**\n",
    "> 1. Normalización: Asegurarse de que las series temporales tengan la misma escala.<br>\n",
    "> 2. Relleno de valores faltantes: Si hay valores faltantes en la serie temporal, se pueden rellenar con el último valor conocido o con un valor estimado.<br>\n",
    "> 3. División en ventanas: Las series temporales pueden dividirse en ventanas deslizantes de un tamaño determinado para crear secuencias más pequeñas.<br>\n",
    "> 4. Creación de características: Las características adicionales, como la media móvil, la varianza, etc., pueden crearse a partir de los datos de la serie temporal existente.<br>\n",
    "> 5. Transformación: Las series temporales pueden transformarse para crear series estacionarias o para reducir la varianza.\n",
    "- **Para Datos de audio:**\n",
    "> 1. Normalización: Asegurarse de que los datos de audio tengan la misma escala.<br>\n",
    "> 2. Reducción de ruido: Eliminar el ruido de los datos de audio.<br>\n",
    "> 3. Extracción de características: Extraer características de los datos de audio, como la frecuencia, la amplitud, el espectro, etc.<br>\n",
    "> 4. Transformación: Los datos de audio se pueden transformar mediante el uso de técnicas de Fourier o Wavelet para reducir la varianza o mejorar la precisión.<br>\n",
    "> 5. Augmentación de datos: Los datos de audio se pueden aumentar mediante la adición de ruido, la variación de la velocidad, etc.<br>\n",
    "- **Para Datos geoespaciales:**\n",
    "> 1. Interpolación: Si hay valores faltantes en los datos geoespaciales, se pueden interpolar para estimar los valores faltantes.<br>\n",
    "> 2. Normalización: Asegurarse de que los datos geoespaciales tengan la misma escala.<br>\n",
    "> 3. Creación de características: Las características adicionales, como la elevación, la temperatura, la humedad, etc., pueden crearse a partir de los datos geoespaciales existentes.<br>\n",
    "> 4. Transformación: Los datos geoespaciales se pueden transformar mediante el uso de técnicas de proyección o reducción de dimensionalidad.<br>\n",
    "- **Para Datos de sensores:**\n",
    "> 1. Normalización: Asegurarse de que los datos de los sensores tengan la misma escala.<br>\n",
    "> 2. Eliminación de valores atípicos: Eliminar los valores extremos de los datos de los sensores.<br>\n",
    "> 3. Creación de características: Las características adicionales, como la media móvil, la varianza, etc., pueden crearse a partir de los datos de los sensores existentes.<br>\n",
    "> 4. Transformación: Los datos de los sensores se pueden transformar mediante el uso de técnicas de Fourier o Wavelet para reducir la varianza o mejorar la precisión.<br>\n",
    "> 5. Augmentación de datos: Los datos de los sensores se pueden aumentar mediante la adición de ruido o la variación de la frecuencia de muestreo.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e6086",
   "metadata": {},
   "source": [
    "# **Arquitectura** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade46ca",
   "metadata": {},
   "source": [
    "## Arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579130e",
   "metadata": {},
   "source": [
    "- **Capa de entrada:** es la primera capa de la red neuronal y recibe los datos de entrada que serán procesados por la red.<br><br>\n",
    "- **Capas ocultas:** son las capas intermedias de la red neuronal que procesan los datos de entrada. Estas capas están formadas por neuronas que reciben información de la capa anterior y la transforman para pasarla a la siguiente capa.<br><br>\n",
    "- **Capa de salida:** es la última capa de la red neuronal y es responsable de producir la salida de la red neuronal. La salida puede ser una clasificación, una predicción o cualquier otra información relevante para el problema que estemos tratando de resolver.<br><br>\n",
    "- **Neuronas:** son los elementos básicos que componen la red neuronal. Cada neurona recibe una entrada y produce una salida. La salida de una neurona se calcula mediante una función de activación que determina si la neurona se activa o no.<br><br>\n",
    "- **Pesos:** son los valores que determinan la fuerza de las conexiones entre las neuronas en la red neuronal. Cada conexión entre dos neuronas tiene un peso asociado que indica la importancia de la información que se transmite a través de esa conexión.<br><br>\n",
    "- **Función de activación:** es la función que determina la salida de una neurona en función de su entrada y de los pesos de las conexiones que la conectan con otras neuronas.<br><br>\n",
    "- **Función de pérdida:** es la función que mide el error de la red neuronal. Esta función se utiliza para ajustar los pesos de la red neuronal durante el entrenamiento.<br><br>\n",
    "- **Función de coste:** es similar a la función de pérdida, pero se utiliza para medir el error en el conjunto de validación o prueba de la red neuronal, en lugar del conjunto de entrenamiento. Esta función se utiliza para evaluar el rendimiento general de la red neuronal.<br><br>\n",
    "- **Batch normalization:** es una técnica utilizada para normalizar la salida de una capa de neuronas. Esta técnica ayuda a mejorar la estabilidad y la velocidad de convergencia de la red neuronal.<br><br>\n",
    "- **Algoritmo de aprendizaje:** es el algoritmo que se utiliza para ajustar los pesos de la red neuronal durante el entrenamiento. El algoritmo más comúnmente utilizado es el descenso del gradiente.<br><br>\n",
    "- **Bias (Sesgo):** son los valores que se añaden a cada neurona para ajustar su salida. El bias se utiliza para asegurarse de que las neuronas se activen incluso si su entrada es cero.<br><br>\n",
    "- **Función de regularización:** es una técnica que se utiliza para evitar el sobreajuste en la red neuronal. Esto implica la introducción de una penalización en la función de pérdida para evitar que los pesos de la red neuronal se vuelvan demasiado grandes durante el entrenamiento. Las técnicas de regularización comunes incluyen el **dropout y la regularización L1 y L2.**<br><br>\n",
    "- **Tasa de aprendizaje:** es un parámetro utilizado por el algoritmo de aprendizaje para determinar la cantidad de ajuste que se realiza en los pesos de la red neuronal en cada iteración del entrenamiento. Una tasa de aprendizaje adecuada es esencial para garantizar que la red neuronal se entrene correctamente.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e29687",
   "metadata": {},
   "source": [
    "## Tipos de Capas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079dc939",
   "metadata": {},
   "source": [
    "- **Capa densa (completamente conectada):**\n",
    "~~~~\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Crear una capa densa con 64 neuronas y función de activación 'relu'\n",
    "dense_layer = Dense(64, activation='relu')\n",
    "~~~~\n",
    "- **Capa de convolución:**\n",
    "~~~~\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Crear una capa de convolución con 32 filtros, tamaño de filtro 3x3 y función de activación 'relu'\n",
    "conv_layer = Conv2D(32, (3, 3), activation='relu')\n",
    "\n",
    "~~~~\n",
    "- **Capa de pooling:**\n",
    "~~~~\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# Crear una capa de pooling con tamaño de pool 2x2\n",
    "pool_layer = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "~~~~\n",
    "- **Capa de dropout:**\n",
    "~~~~\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Crear una capa de dropout con una tasa de abandono del 20%\n",
    "dropout_layer = Dropout(0.2)\n",
    "~~~~\n",
    "- **Estas capas se pueden apilar y combinar** para crear una arquitectura de red neuronal personalizada. Por ejemplo, la siguiente línea de código crea una red neuronal con una capa densa de entrada, dos capas densas ocultas, una capa de dropout y una capa de salida densa:\n",
    "~~~~\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(output_dim, activation='softmax')\n",
    "])\n",
    "~~~~\n",
    "Donde input_dim y output_dim son las dimensiones de entrada y salida de la red neuronal, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3542d0f",
   "metadata": {},
   "source": [
    "## **Tipos de Redes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3558b9b",
   "metadata": {},
   "source": [
    "- **Redes feedforward:** Son las redes neuronales más simples y comunes. Se componen de una capa de entrada, una o más capas ocultas y una capa de salida. Las señales fluyen solo en una dirección, desde la entrada hasta la salida. Estas redes son comúnmente utilizadas para tareas de clasificación y regresión.\n",
    "<img src=https://d1.awsstatic.com/whatisimg/intro-gluon-1%20(1).ac2f31378926b5f99a4ba9d741c4aebe3b7a29e2.png width=\"400\"><br><br>\n",
    "- **Redes recurrentes:** En este tipo de red neuronal, las neuronas tienen conexiones recurrentes, lo que significa que pueden recibir información de otras neuronas en capas anteriores. Las redes recurrentes son útiles para tareas en las que la entrada es una secuencia de datos, como en el procesamiento del lenguaje natural o en la predicción de series de tiempo.<img src=https://abdatum.com/media/images/neuronas-recurrentes.jpeg width=\"400\"><br><br>\n",
    "- **Redes convolucionales:** Estas redes están diseñadas específicamente para tareas de visión por computadora y procesamiento de imágenes. Utilizan capas convolucionales para extraer características de la imagen y reducir la dimensionalidad antes de pasar la información a capas completamente conectadas para la clasificación final.<img src=https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/CNN-08.png width=\"400\"><br><br>\n",
    "- **Redes autoencoder:** Son una arquitectura de red neuronal que se utiliza para el aprendizaje no supervisado de características. Consisten en una capa de entrada, una o más capas ocultas y una capa de salida. El objetivo del autoencoder es aprender a codificar la entrada en una representación de menor dimensión y luego reconstruir la entrada original a partir de esta representación comprimida.<img src=http://www.cs.us.es/~fsancho/images/2020-03/vaearc.png width=\"400\"><br><br>\n",
    "- **Redes generativas adversarias (GAN):** Esta arquitectura de red neuronal consta de dos redes: un generador y un discriminador. El generador crea muestras que parecen reales, mientras que el discriminador intenta distinguir entre las muestras reales y las generadas. El objetivo es que el generador aprenda a generar muestras que engañen al discriminador y se parezcan lo más posible a las muestras reales.<img src=https://www.iartificial.net/wp-content/uploads/2019/03/redes-generativas-adversarias-1024x291.webp width=\"400\"><br><br>\n",
    "- **Redes neuronales de Memoria a Corto y Largo Plazo (LSTM):** son una variante de las redes recurrentes que se utilizan principalmente para modelar secuencias de datos con dependencias a largo plazo. Las LSTM tienen una estructura de celda que les permite almacenar información a largo plazo y decidir cuándo y cómo olvidarla.<img src=https://www.researchgate.net/profile/Xuan_Hien_Le2/publication/334268507/figure/fig8/AS:788364231987201@1564972088814/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan.png width=\"400\"><br><br>\n",
    "- **Redes neuronales Transformer:** se han utilizado en aplicaciones de procesamiento de lenguaje natural. La arquitectura Transformer utiliza una estructura de atención que permite a la red tomar en cuenta diferentes partes del texto simultáneamente, lo que la hace más eficiente que otras arquitecturas en el procesamiento de secuencias largas.<img src=https://blogs.nvidia.com/wp-content/uploads/2022/03/Transformer-apps.jpg width=\"400\"><br><br>\n",
    "- **Redes neuronales Residuales:** se utilizan para construir redes profundas más fáciles de entrenar. Las redes residuales contienen conexiones directas entre capas, lo que permite que la información fluya directamente desde la entrada a través de múltiples capas sin ser alterada por la función de activación de la capa.<img src=https://www.iartificial.net/wp-content/uploads/2019/03/redes-generativas-adversarias-1024x291.webp width=\"400\"><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95227a",
   "metadata": {},
   "source": [
    "## Funciones de Activación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb976428",
   "metadata": {},
   "source": [
    "<img src=https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39110a53",
   "metadata": {},
   "source": [
    "Una función de activación es una función matemática que se aplica a la salida de una neurona en una red neuronal artificial. Su función principal es introducir no-linealidad en la red neuronal, permitiendo así que la red pueda modelar funciones complejas.<br><br>\n",
    "\n",
    "- **Función de activación Sigmoide:**<br><br>\n",
    "<img src=https://ml4a.github.io/images/figures/sigmoid.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas donde se necesita una salida en un rango limitado de valores, como en la clasificación binaria o en problemas de regresión que tienen valores de salida acotados.\n",
    "> **Ventajas:** La función sigmoide es suave y diferenciable, lo que facilita el entrenamiento de la red. También tiene una interpretación probabilística, lo que puede ser útil en algunos casos.<br><br>\n",
    "> **Desventajas:** La función sigmoide puede sufrir el problema del gradiente desvaneciente, lo que dificulta el entrenamiento de redes profundas. Además, la función tiene un rango limitado de valores de salida, lo que puede limitar la capacidad de la red para modelar funciones complejas.\n",
    "\n",
    "- **Función de activación ReLU:**<br><br>\n",
    "<img src=https://ml4a.github.io/images/figures/relu.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas donde se necesita una salida no lineal, como en la clasificación o la regresión, especialmente en redes neuronales profundas.\n",
    ">**Ventajas:** La función ReLU es simple y rápida de calcular. También es no lineal, lo que permite que la red pueda modelar funciones complejas.<br><br>\n",
    "> **Desventajas:** La función ReLU puede tener problemas con gradientes negativos, lo que puede llevar a que algunas neuronas queden inactivas (problema de \"neuronas muertas\"). Además, la función no es suave, lo que puede dificultar el entrenamiento de la red en algunos casos.\n",
    "\n",
    "- **Función de activación Tangente Hiperbólica:**<br><br>\n",
    "<img src=https://www.researchgate.net/profile/Ignacio-Arroyo-Fernandez/publication/264899327/figure/fig18/AS:614376570642432@1523490197254/Figura-215-Funcion-de-activacion-tangente-hiperbolica-para-los-nodos-de-procesamiento.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas donde se necesita una salida en un rango de valores simétrico, como en la clasificación o la regresión.\n",
    "> **Ventajas:** La función tangente hiperbólica es suave y diferenciable, lo que facilita el entrenamiento de la red. También tiene un rango de valores de salida más amplio que la función sigmoide, lo que puede permitir que la red modele funciones más complejas.<br><br>\n",
    "> **Desventajas:** La función tangente hiperbólica también puede sufrir el problema del gradiente desvaneciente en redes profundas. Además, la función puede tener problemas con valores grandes de entrada, lo que puede llevar a saturación de la neurona.\n",
    "\n",
    "- **Función de activación Softmax:**<br><br>\n",
    "<img src=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_11.56.35_PM_yh1VO82.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas de clasificación de varias clases, donde se necesita una salida que represente la probabilidad de pertenecer a cada clase. Por esta razón es que la función softmax es típicamente utilizada para \"filtrar\" un conjunto de valores que se encuentren por debajo de un valor máximo establecido.\n",
    "> **Ventajas:** La función Softmax es útil para problemas de clasificación, ya que normaliza las salidas de las neuronas para que sumen 1, lo que permite interpretar la salida como una distribución de probabilidad sobre las clases.<br><br>\n",
    "> **Desventajas:** La función Softmax puede sufrir problemas de estabilidad numérica si las entradas son muy grandes o muy pequeñas. También puede ser propensa al sobreajuste en problemas de clasificación con muchas clases.\n",
    "\n",
    "- **Función de activación Leaky ReLU:**<br><br>\n",
    "<img src=https://numerentur.org/wp-content/uploads/2019/06/PRELU4.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas donde se necesita una salida no lineal que evite el problema de \"neuronas muertas\".\n",
    "> **Ventajas:** La función Leaky ReLU es una variante de la función ReLU que evita el problema de \"neuronas muertas\" al permitir gradientes negativos. También es simple y rápida de calcular.<br><br>\n",
    "> **Desventajas:** La función Leaky ReLU puede tener problemas de saturación para valores grandes de entrada, lo que puede limitar su capacidad para modelar funciones complejas. Además, el valor de la constante alpha debe ajustarse correctamente para obtener buenos resultados.\n",
    "\n",
    "- **Función de activación ELU:**<br><br>\n",
    "<img src=https://numerentur.org/wp-content/uploads/2019/06/ELU1.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas donde se necesita una salida no lineal que evite el problema de \"neuronas muertas\" y reduzca el sobreajuste.\n",
    "> **Ventajas:** La función ELU es una función suave y diferenciable que evita el problema de \"neuronas muertas\" y puede ayudar a reducir el sobreajuste en algunas redes. También tiene un rango de valores de salida más amplio que la función ReLU.<br><br>\n",
    "> **Desventajas:** La función ELU puede ser más lenta de calcular que la función ReLU y puede sufrir problemas de saturación para valores grandes de entrada.\n",
    "\n",
    "- **Función de activación Swish:**<br><br>\n",
    "<img src=https://i.ytimg.com/vi/JewUzs5XugE/maxresdefault.jpg width=\"400\"><br><br>\n",
    " La función Swish es una función suave, no lineal y diferenciable que se asemeja a la función ReLU pero con una saturación suave. A diferencia de la función GELU, la función Swish tiene una forma similar a una S.\n",
    "> **Ventajas:** La función Swish es una función suave y diferenciable que ha demostrado tener un mejor rendimiento que la función ReLU en algunas redes neuronales. También es fácil de calcular y tiene una forma de curva similar a la función sigmoide.<br><br>\n",
    "> **Desventajas:** La función Swish puede ser más lenta de calcular que la función ReLU y puede sufrir problemas de saturación para valores grandes de entrada.\n",
    "\n",
    "- **Función de activación GELU:**<br><br>\n",
    "<img src=https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg width=\"400\"><br><br>\n",
    "La función GELU (Gaussian Error Linear Unit) es una función de activación propuesta recientemente que se utiliza en redes neuronales profundas. La función GELU es una función suave, no lineal y diferenciable que se asemeja a la función ReLU pero con una curva más suave y una saturación más suave.\n",
    "> **Ventajas:** La función GELU es una función suave y diferenciable que ha demostrado tener un mejor rendimiento que la función ReLU en algunas redes neuronales. También es fácil de calcular y tiene una forma de curva similar a la función sigmoide.<br><br>\n",
    "> **Desventajas:** La función GELU puede ser más lenta de calcular que la función ReLU y puede requerir una mayor capacidad de cómputo para entrenar modelos grandes.\n",
    "\n",
    "- **Función de activación Lineal:**<br><br>\n",
    "<img src=https://iq.opengenus.org/content/images/2021/11/Activation-2.png width=\"400\"><br><br>\n",
    "Se utiliza en problemas donde se necesita una salida lineal en lugar de una no lineal, como en la regresión.\n",
    "> **Ventajas:** La función lineal es simple y fácil de calcular. Es útil en algunos casos donde se necesita una salida lineal en lugar de una no lineal.<br><br>\n",
    "> **Desventajas:** La función lineal no introduce no linealidad en la red y, por lo tanto, no es adecuada para la mayoría de las aplicaciones de redes neuronales.\n",
    "\n",
    "- **Y MÁS...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f45bd8",
   "metadata": {},
   "source": [
    "### **RESUMEN**\n",
    "<img src=https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/62b18a8dc83132e1a479b65d_neural-network-activation-function-cheat-sheet.jpeg width=\"800\"><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57761dc4",
   "metadata": {},
   "source": [
    "## Funciones de Pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e19c7",
   "metadata": {},
   "source": [
    "## Algoritmos de Aprendizaje "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb449ce",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaada9d6",
   "metadata": {},
   "source": [
    "## Optimizadores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068bc4c",
   "metadata": {},
   "source": [
    "Los optimizadores de redes neuronales son algoritmos utilizados para ajustar los pesos y los sesgos de las neuronas en una red neuronal durante el proceso de entrenamiento, con el objetivo de minimizar la función de pérdida y mejorar el rendimiento del modelo.<br><br>\n",
    "- **Parámetros de Código**:<br><br>\n",
    "    **loss_func:** es la función de pérdida que se utiliza para evaluar el error de la red neuronal en función de las etiquetas verdaderas y las etiquetas predichas.<br><br>\n",
    "    **inputs:** son los datos de entrada (características) que se utilizan para entrenar la red neuronal.<br><br>\n",
    "    **targets:** son las etiquetas verdaderas correspondientes a los datos de entrada.<br><br>\n",
    "    **weights:** son los pesos de la red neuronal que se actualizan durante el entrenamiento para minimizar la función de pérdida.<br><br>\n",
    "    **biases:** son los sesgos de la red neuronal que se actualizan durante el entrenamiento para minimizar la función de pérdida.<br><br>\n",
    "Durante el entrenamiento, la red neuronal ajusta los pesos y sesgos para minimizar la función de pérdida. Para ello, se utilizan los datos de entrada (inputs) y las etiquetas verdaderas (targets) para calcular el error de la red neuronal en función de los pesos y sesgos actuales. A continuación, se utiliza un optimizador para actualizar los pesos y sesgos en función de la magnitud del error y la tasa de aprendizaje especificada. Este proceso se repite iterativamente hasta que la red neuronal alcance una precisión aceptable.<br><br>\n",
    "- **Tipos**:<br><br>\n",
    ">**Gradient Descent (descenso de gradiente):** es el optimizador más básico y utilizado en la mayoría de los modelos de aprendizaje profundo. El algoritmo se basa en calcular el gradiente de la función de pérdida y actualizar los pesos y sesgos de las neuronas en la dirección opuesta al gradiente.<br><br>\n",
    "~~~~\n",
    "                    # Inicializar los pesos y sesgos\n",
    "                    weights, biases = initialize_params()\n",
    "\n",
    "                    # Calcular el gradiente de la función de pérdida\n",
    "                    grads = calculate_gradient(loss_func, inputs, targets, weights, biases)\n",
    "\n",
    "                    # Actualizar los pesos y sesgos en la dirección opuesta al gradiente\n",
    "                    weights -= learning_rate * grads['weights']\n",
    "                    biases -= learning_rate * grads['biases']\n",
    "~~~~\n",
    ">**Stochastic Gradient Descent (SGD):** es similar al descenso de gradiente, pero se utiliza una muestra aleatoria de los datos de entrenamiento para calcular el gradiente y actualizar los pesos y sesgos. Este optimizador es más rápido y escalable que el descenso de gradiente.<br><br>\n",
    "~~~~\n",
    "                    # Inicializar los pesos y sesgos\n",
    "                    weights, biases = initialize_params()\n",
    "\n",
    "                    # Repetir para cada muestra en los datos de entrenamiento\n",
    "                    for i in range(num_samples):\n",
    "                        # Seleccionar una muestra aleatoria\n",
    "                        sample = random.choice(data)\n",
    "\n",
    "                        # Calcular el gradiente de la función de pérdida para la muestra seleccionada\n",
    "                        grads = calculate_gradient(loss_func, sample['inputs'], sample['targets'], weights, biases)\n",
    "\n",
    "                        # Actualizar los pesos y sesgos en la dirección opuesta al gradiente\n",
    "                        weights -= learning_rate * grads['weights']\n",
    "                        biases -= learning_rate * grads['biases']\n",
    "~~~~\n",
    ">**Adaptive Moment Estimation (Adam):** es un optimizador popular en la industria del aprendizaje profundo que combina las ventajas del descenso de gradiente y el SGD. El algoritmo utiliza una tasa de aprendizaje adaptativa para actualizar los pesos y sesgos en función del historial de gradiente.<br><br>\n",
    "~~~~\n",
    "                    # Inicializar los pesos y sesgos\n",
    "                    weights, biases = initialize_params()\n",
    "\n",
    "                    # Inicializar los momentos del gradiente\n",
    "                    m = 0\n",
    "                    v = 0\n",
    "\n",
    "                    # Repetir para cada época de entrenamiento\n",
    "                    for epoch in range(num_epochs):\n",
    "                        # Calcular el gradiente de la función de pérdida\n",
    "                        grads = calculate_gradient(loss_func, inputs, targets, weights, biases)\n",
    "\n",
    "                        # Actualizar los momentos del gradiente\n",
    "                        m = beta1 * m + (1 - beta1) * grads['weights']\n",
    "                        v = beta2 * v + (1 - beta2) * (grads['weights'] ** 2)\n",
    "\n",
    "                        # Corregir el sesgo de los momentos del gradiente\n",
    "                        m_hat = m / (1 - beta1 ** epoch)\n",
    "                        v_hat = v / (1 - beta2 ** epoch)\n",
    "\n",
    "                        # Actualizar los pesos y sesgos en función de los momentos corregidos del gradiente\n",
    "                        weights -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "                        biases -= learning_rate * grads['biases']\n",
    "~~~~\n",
    ">**Root Mean Square Propagation (RMSProp)**: es otro optimizador que adapta la tasa de aprendizaje en función del historial de gradiente. El algoritmo utiliza la media cuadrática de los gradientes pasados para ajustar la tasa de aprendizaje.<br><br>\n",
    "~~~~\n",
    "                    # Inicializar los pesos y sesgos\n",
    "                    weights, biases = initialize_params()\n",
    "\n",
    "                    # Inicializar la media cuadrática del gradiente\n",
    "                    r = 0\n",
    "\n",
    "                    # Repetir para cada época de entrenamiento\n",
    "                    for epoch in range(num_epochs):\n",
    "                        # Calcular el gradiente de la función de pérdida\n",
    "                        grads = calculate_gradient(loss_func, inputs, targets, weights, biases)\n",
    "\n",
    "                        # Actualizar la media cuadrática del gradiente\n",
    "                        r = beta * r + (1 - beta) * (grads['weights'] ** 2)\n",
    "\n",
    "                        # Actualizar los pesos y sesgos en función de la tasa de aprendizaje adaptativa\n",
    "                        weights -= learning_rate * grads['weights'] / (np.sqrt(r) + epsilon)\n",
    "                        biases -= learning_rate * grads['biases']\n",
    "~~~~\n",
    ">**Adagrad:** es un optimizador que ajusta la tasa de aprendizaje de cada parámetro de manera individual en función de su historial de gradiente. El algoritmo da más importancia a los parámetros con gradientes raros y menos importancia a los parámetros con gradientes comunes.<br><br>\n",
    "~~~~\n",
    "                    # Inicializar los pesos y sesgos\n",
    "                    weights, biases = initialize_params()\n",
    "\n",
    "                    # Inicializar el acumulador del gradiente\n",
    "                    cache = 0\n",
    "\n",
    "                    # Repetir para cada época de entrenamiento\n",
    "                    for epoch in range(num_epochs):\n",
    "                        # Calcular el gradiente de la función de pérdida\n",
    "                        grads = calculate_gradient(loss_func, inputs, targets, weights, biases)\n",
    "\n",
    "                        # Actualizar el acumulador del gradiente\n",
    "                        cache += grads['weights'] ** 2\n",
    "\n",
    "                        # Actualizar los pesos y sesgos en función de la tasa de aprendizaje adaptativa\n",
    "                        weights -= learning_rate * grads['weights'] / (np.sqrt(cache) + epsilon)\n",
    "                        biases -= learning_rate * grads['biases']\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc31569",
   "metadata": {},
   "source": [
    "# **Diseño de una Red Neuronal** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81094de",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1656fcc",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981df97b",
   "metadata": {},
   "source": [
    "## Nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448fd05",
   "metadata": {},
   "source": [
    "## Funciones (activación, pérdidas, costes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489d368",
   "metadata": {},
   "source": [
    "## Elección de Capas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4febd3b",
   "metadata": {},
   "source": [
    "- **Comenzar con una cantidad conservadora de neuronas y aumentarla gradualmente:** una estrategia común es comenzar con una cantidad conservadora de neuronas y aumentarla gradualmente hasta que se alcance un buen rendimiento. Esto permite explorar diferentes tamaños de red y encontrar el equilibrio adecuado entre la complejidad y el rendimiento.<br><br>\n",
    "- **Basado en la complejidad del problema:** la elección del número de neuronas también puede basarse en la complejidad del problema. Problemas más complejos pueden requerir una red neuronal más grande con más neuronas, mientras que problemas más simples pueden requerir una red más pequeña.<br><br>\n",
    "- **Basado en la arquitectura de la red:** la elección del número de neuronas también puede depender de la arquitectura de la red neuronal. Por ejemplo, una red convolucional puede tener un número diferente de neuronas en cada capa que una red completamente conectada.<br><br>\n",
    "- **Uso de técnicas de selección de características:** otra estrategia común es utilizar técnicas de selección de características para reducir la cantidad de entradas que entran en la red neuronal. Esto puede permitir el uso de una red neuronal más pequeña con menos neuronas.<br><br>\n",
    "- **Uso de técnicas de validación cruzada:** la validación cruzada puede ser utilizada para evaluar el rendimiento de la red neuronal para diferentes tamaños de red y seleccionar el tamaño que ofrece el mejor rendimiento.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1a10f",
   "metadata": {},
   "source": [
    "## Función de Regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e4b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d2c369",
   "metadata": {},
   "source": [
    "# Compilación y Guardado del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4df50f",
   "metadata": {},
   "source": [
    "# **Contextualizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8139cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "485.575px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
