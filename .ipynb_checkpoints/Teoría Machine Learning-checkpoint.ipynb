{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b4d551",
   "metadata": {},
   "source": [
    "# **¿Cómo crear un Modelo de Machine Learning.?** \n",
    "\n",
    "*by: Alberto Padilla Nieto*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b152c8b",
   "metadata": {},
   "source": [
    " ## Pasos para modelo Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2aa41",
   "metadata": {},
   "source": [
    "1. Importar las **librerías** necesarias para el análisis de datos, preprocesamiento y modelado, como Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, etc.\n",
    "\n",
    "\n",
    "2. **Cargar y explorar los datos** que se utilizarán para entrenar y evaluar el modelo.\n",
    "\n",
    "\n",
    "3. **Preprocesar los datos**, lo que puede incluir la limpieza, transformación y normalización o  cualquier test estadístico de los datos, la selección de características, la codificación de variables categóricas, la división de los datos en conjuntos de entrenamiento y prueba, etc.\n",
    "\n",
    "\n",
    "4. Seleccionar y entrenar un **modelo de machine learning** que se ajuste a los datos y el problema a resolver, como regresión lineal, árboles de decisión, redes neuronales, etc.\n",
    "\n",
    "\n",
    "5. Evaluar el **rendimiento del modelo** utilizando métricas relevantes, como la precisión, la recall, el F1-score, la matriz de confusión, etc.\n",
    "\n",
    "\n",
    "6. **Ajustar el modelo** utilizando técnicas como la búsqueda de hiperparámetros, la validación cruzada, el ajuste de peso de clases y/o el uso de algoritmos robustos como SVM o Redes Neuronales Profundas.\n",
    "\n",
    "\n",
    "7. Realizar pruebas exhaustivas del modelo para asegurarse de que pueda manejar situaciones extremas como **outliers y datos ruidosos.**\n",
    "\n",
    "\n",
    "8. Utilizar el modelo entrenado para **realizar predicciones** sobre nuevos datos y/o para resolver el problema que se planteó inicialmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a7c9f",
   "metadata": {},
   "source": [
    "# **Librerias** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6b2f2",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820580d",
   "metadata": {},
   "source": [
    "- **Librerías gráficas:** Estas librerías permiten la visualización de datos y la creación de gráficos y diagramas para ayudar en el análisis de datos. Matplotlib.pyplot es una de las librerías más populares para visualización de datos en Python, Seaborn se enfoca en gráficos estadísticos más sofisticados y Pandas.plotting proporciona algunas herramientas de visualización integradas en Pandas para análisis de datos.\n",
    "~~~~\n",
    "                        import matplotlib.pyplot, \n",
    "                        import seaborn, \n",
    "                        import pandas.plotting\n",
    "~~~~\n",
    "\n",
    "- **Librerías de dataframes:** Pandas es una librería muy útil para el análisis de datos y la manipulación de datos en forma de tablas y dataframes. Numpy también se utiliza frecuentemente en combinación con Pandas para realizar operaciones matemáticas en los datos.\n",
    "~~~~\n",
    "                        import pandas\n",
    "                        import numpy  \n",
    "~~~~\n",
    "\n",
    "- **Librerías del sistema:** Estas librerías están relacionadas con la manipulación del sistema operativo y los archivos. Os proporciona funciones para trabajar con los sistemas operativos, Base64 se utiliza para la codificación y decodificación de datos, io.BytesIO proporciona un buffer de bytes para leer y escribir datos en memoria, PIL.Image permite la manipulación de imágenes, IPython.display muestra objetos en formato HTML en notebooks de Jupyter, tkinter es una biblioteca gráfica para interfaces de usuario en Python y tkinter.font proporciona funciones para trabajar con fuentes.\n",
    "~~~~\n",
    "                        import o \n",
    "                        import base6 \n",
    "                        import io.BytesI \n",
    "                        import PIL.Imag \n",
    "                        import IPython.displa \n",
    "                        import tkinte \n",
    "                        import tkinter.font  \n",
    "~~~~\n",
    "\n",
    "- **Librerías de informes:** Estas librerías se utilizan para generar informes y realizar análisis estadísticos. Statsmodels.api proporciona funciones para realizar análisis de regresión y otros modelos estadísticos, statsmodels.formula.api proporciona una interfaz más sencilla para trabajar con modelos estadísticos, Pandas_profiling genera informes automatizados sobre los datos, y statsmodels.stats.outliers_influence proporciona funciones para analizar los puntos atípicos en los datos.\n",
    "~~~~\n",
    "                        import statsmodels.api \n",
    "                        import statsmodels.formula.api \n",
    "                        import pandas_profiling \n",
    "                        import statsmodels.stats.outliers_influence\n",
    "~~~~\n",
    "\n",
    "- **Librerías para datos de entrenamiento y prueba:** sklearn.model_selection proporciona funciones para dividir los datos en conjuntos de entrenamiento y prueba para su uso en modelos de aprendizaje automático.\n",
    "~~~~\n",
    "                        import sklearn.model_selection\n",
    "~~~~\n",
    "\n",
    "\n",
    "- **Librerías para modelos lineales:** sklearn.linear_model y sklearn.preprocessing se utilizan para trabajar con modelos lineales como regresión lineal, regresión logística, etc.\n",
    "~~~~\n",
    "                        import sklearn.linear_model \n",
    "                        import sklearn.preprocessing\n",
    "~~~~\n",
    "\n",
    "- **Librerías para modelos logísticos:** sklearn.linear_model y sklearn.metrics se utilizan para trabajar con modelos logísticos.\n",
    "~~~~\n",
    "                        import sklearn.linear_model \n",
    "                        import sklearn.metrics\n",
    "~~~~\n",
    "\n",
    "- **Librerías para modelos de clasificación:** sklearn.preprocessing, sklearn.model_selection y sklearn.metrics se utilizan para trabajar con modelos de clasificación.\n",
    "~~~~\n",
    "                        import sklearn.preprocessing \n",
    "                        import sklearn.model_selection \n",
    "                        import sklearn.metrics\n",
    "~~~~\n",
    "\n",
    "- **Librerías para KNN:** sklearn.neighbors proporciona funciones para trabajar con el algoritmo K-Nearest Neighbors.\n",
    "~~~~\n",
    "                        import sklearn.neighbors\n",
    "~~~~\n",
    "\n",
    "- **Librerías para árboles de decisión:** sklearn.tree proporciona funciones para trabajar con modelos de árboles de decisión. \n",
    "~~~~\n",
    "                        import sklearn.tree\n",
    "~~~~\n",
    "\n",
    "- **Librerías para generación de datos:** sklearn.datasets proporciona funciones para generar datos de prueba y entrenamiento de diferentes tipos de datos.\n",
    "~~~~\n",
    "                        import sklearn.datasets\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8a21a",
   "metadata": {},
   "source": [
    "# **Carga y Exploración de Datos** ([PandasProfiling](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94184f1",
   "metadata": {},
   "source": [
    "**Pandas profiling** \n",
    "\n",
    "Es una biblioteca de Python para generar informes descriptivos rápidos de conjuntos de datos de Pandas. Al utilizar pandas profiling, se puede generar automáticamente un informe detallado que incluye estadísticas básicas de resumen, distribución de valores, valores faltantes, correlaciones y mucho más. Este informe puede ser útil para obtener una visión general rápida y completa de los datos, lo que puede ahorrar tiempo en la exploración manual de los datos. Además, el informe generado por pandas profiling se presenta en un formato visualmente atractivo y fácil de entender.\n",
    "~~~\n",
    "profile=ProfileReport(df)\n",
    "~~~\n",
    "    \n",
    "    \n",
    "    \n",
    "   - **Extracción de Datos del informe**\n",
    "\n",
    "    El informe generado incluye una sección llamada \"Variables\" que lista todas las variables en el conjunto de datos y proporciona información detallada sobre cada una. Para cada variable, se muestra una tabla con estadísticas resumidas, una distribución de frecuencia y un gráfico que representa la distribución de valores.\n",
    "    En cada sección del informe, se proporcionan enlaces para descargar los datos utilizados para crear las visualizaciones y tablas. Por lo tanto, es posible descargar los datos utilizados para generar el informe y trabajar con ellos en un marco de datos de pandas regular. Esto puede ser especialmente útil si se desea realizar análisis más detallados o utilizar técnicas de modelado predictivo avanzadas en los datos.\n",
    "    \n",
    "**OTROS**\n",
    "\n",
    "**df.info():** Este método de pandas se utiliza para obtener información sobre un DataFrame, como la cantidad de filas y columnas, los nombres de las columnas, el tipo de datos de cada columna, la cantidad de valores no nulos en cada columna, entre otros. \n",
    "~~~\n",
    "df.info()\n",
    "\n",
    "~~~\n",
    "\n",
    "**df.describe():** Este método de pandas se utiliza para obtener estadísticas descriptivas sobre un DataFrame, como la media, la mediana, el mínimo y el máximo de cada columna. \n",
    "~~~\n",
    "df.describe(include=[])\n",
    "\n",
    "El parámetro include se puede establecer con uno o varios tipos de datos como una lista, y se pueden utilizar los siguientes valores:\n",
    "\n",
    "                'object': incluye columnas de tipo objeto o string.\n",
    "                'int': incluye columnas de tipo entero.\n",
    "                'float': incluye columnas de tipo flotante.\n",
    "                'bool': incluye columnas de tipo booleano.\n",
    "                'all': incluye todas las columnas.\n",
    "~~~\n",
    "\n",
    "**df.head():** Este método de pandas se utiliza para obtener las primeras filas de un DataFrame. Por defecto, devuelve las primeras 5 filas, pero se puede especificar un número diferente.\n",
    "~~~\n",
    "df.head(10)  # devuelve las primeras 10 filas\n",
    "~~~\n",
    "\n",
    "**df.tail():** Este método de pandas se utiliza para obtener las últimas filas de un DataFrame. Por defecto, devuelve las últimas 5 filas, pero se puede especificar un número diferente. \n",
    "~~~\n",
    "df.tail(10)  # devuelve las últimas 10 filas\n",
    "~~~\n",
    "\n",
    "**df.shape:** Este atributo de pandas devuelve una tupla con la cantidad de filas y columnas de un DataFrame. Ejemplo:\n",
    "~~~\n",
    "print(df.shape)  # muestra la cantidad de filas y columnas\n",
    "~~~\n",
    "\n",
    "**df.sample():** Este atributo devuelve un numero aleatorio de filas del df.\n",
    "\n",
    "~~~\n",
    "df.sample(10)    # Muestra 10 filas aleatorias\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c24cf3",
   "metadata": {},
   "source": [
    "# **Pre-procesamiento de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff62eb",
   "metadata": {},
   "source": [
    "- **Variables numéricas:** estas variables representan números y pueden ser continuas o discretas.\n",
    "\n",
    "\n",
    "- **Variables categóricas:** estas variables representan categorías o clases y pueden ser nominales o ordinales.\n",
    "\n",
    "\n",
    "- **Variables de texto:** estas variables representan texto o cadenas de caracteres.\n",
    "\n",
    "\n",
    "- **Variables booleanas:** estas variables representan valores binarios verdadero/falso o 0/1.\n",
    "\n",
    "\n",
    "- **Variables de fecha/hora:** estas variables representan fechas y/o horas.\n",
    "\n",
    "\n",
    "- **Variables de objetos:** estas variables pueden contener diferentes tipos de datos, como listas, diccionarios y otros objetos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5be89",
   "metadata": {},
   "source": [
    "## **Limpieza de datos:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53e443",
   "metadata": {},
   "source": [
    "### Eliminación de valores atípicos (Outliers).\n",
    "En general, se pueden considerar tres escenarios en los que se podría optar por eliminar los outliers:\n",
    "\n",
    "- **Errores de medición:** Si los outliers son el resultado de errores de medición o de entrada de datos, pueden eliminarse para obtener una imagen más precisa del conjunto de datos.\n",
    "\n",
    "\n",
    "- **Análisis estadístico:** En algunos casos, los outliers pueden afectar negativamente el análisis estadístico, como el cálculo de la media o la desviación estándar. En estos casos, se podría optar por eliminar los outliers para obtener estadísticas más representativas.\n",
    "\n",
    "\n",
    "- **Modelos de aprendizaje automático:** Algunos modelos de aprendizaje automático pueden verse afectados negativamente por la presencia de outliers, especialmente los modelos lineales. En estos casos, la eliminación de outliers podría mejorar el rendimiento del modelo.\n",
    "\n",
    "Es importante tener en cuenta que la elección del método de eliminación de outliers depende del conjunto de datos y del problema en cuestión. Además, es importante realizar un análisis detallado de los datos antes de eliminar outliers para asegurarse de que se están eliminando solo aquellos valores que son realmente atípicos y no aquellos que son simplemente **extremos pero importantes para el análisis**.\n",
    "\n",
    "**criterios y técnicas**\n",
    "\n",
    "**Corte de valores extremos:** Este método implica la eliminación de los valores que se encuentran por encima o por debajo de ciertos umbrales definidos por el usuario. Por ejemplo, se pueden eliminar todos los valores que estén por encima del percentil 95 o por debajo del percentil 5 de la distribución de los datos.\n",
    "~~~\n",
    "# Eliminar valores extremos\n",
    "df = df[(df['x'] >= df['x'].quantile(0.05)) & (df['x'] <= df['x'].quantile(0.95))]\n",
    "\n",
    "En este ejemplo, se utiliza el método quantile() para calcular el percentil 5 y el percentil 95 de la columna x, y luego se eliminan todos los valores que estén por debajo del percentil 5 o por encima del percentil 95.\n",
    "~~~\n",
    "\n",
    "\n",
    "**Z-score:** Este método implica el cálculo del z-score para cada punto de datos y la eliminación de los puntos que se encuentran por encima de un umbral determinado. El z-score se calcula como la diferencia entre el punto de datos y la media, dividida por la desviación estándar.\n",
    "~~~\n",
    "# Calcular el z-score para cada punto de datos\n",
    "df['z_score'] = np.abs((df['x'] - df['x'].mean()) / df['x'].std())\n",
    "\n",
    "# Eliminar valores extremos\n",
    "df = df[df['z_score'] < 3]\n",
    "\n",
    "En este ejemplo, se calcula el z-score para cada punto de datos en la columna x y se elimina cualquier valor que tenga un z-score superior a 3.\n",
    "~~~\n",
    "**Análisis multivariante:** El análisis multivariante puede detectar patrones inusuales en los datos, lo que puede indicar la presencia de outliers. Por ejemplo, se pueden utilizar técnicas de análisis de componentes principales (PCA) para reducir la dimensionalidad de los datos y detectar patrones inusuales.\n",
    "~~~\n",
    "# Reducir la dimensionalidad utilizando PCA\n",
    "pca = PCA(n_components=1)\n",
    "df['pca'] = pca.fit_transform(df)\n",
    "\n",
    "# Eliminar valores extremos\n",
    "df = df[np.abs(df['pca']) < 3 * np.std(df['pca'])]\n",
    "\n",
    "En este ejemplo, se utiliza el análisis de componentes principales (PCA) para reducir la dimensionalidad del DataFrame a una sola columna y luego se eliminan cualquier valor que tenga un valor absoluto superior a 3 veces la desviación estándar de la columna reducida.\n",
    "~~~\n",
    "**Filtrado de mediana:** El filtrado de mediana implica la sustitución de los valores extremos por la mediana del conjunto de datos. Este método es útil cuando se desea mantener la forma de la distribución original de los datos.\n",
    "~~~\n",
    "# Calcular la mediana\n",
    "median = np.median(df['x'])\n",
    "\n",
    "# Reemplazar los valores extremos con la mediana\n",
    "df.loc[df['x'] > 3 * median, 'x'] = median\n",
    "df.loc[df['x'] < 0.3 * median, 'x'] = median\n",
    "\n",
    "En este ejemplo, se calcula la mediana de la columna x y se reemplazan cualquier valor por encima de 3 veces la mediana o por debajo de 0.3 veces la mediana con la mediana.\n",
    "~~~\n",
    "**Modelos robustos:** Los modelos robustos son menos sensibles a los valores atípicos, por lo que pueden ser utilizados para identificar y eliminar los outliers. Por ejemplo, se pueden utilizar modelos de regresión robustos en lugar de modelos de regresión lineal estándar.\n",
    "~~~\n",
    "df = pd.read_csv('archivo.csv')\n",
    "\n",
    "Calcular el cuantil 0.95 de la columna 'B' para identificar los outliers\n",
    "q = df['B'].quantile(0.95)\n",
    "\n",
    "Eliminar los valores que estén por encima del cuantil 0.95\n",
    "df = df[df['B'] < q]\n",
    "\n",
    "Mostrar el DataFrame resultante sin outliers\n",
    "print(df)\n",
    "~~~\n",
    "**...Y MÁS**\n",
    "~~~\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a1f0f",
   "metadata": {},
   "source": [
    "### Valores Duplicados.\n",
    "\n",
    "En particular, los valores duplicados pueden afectar negativamente la calidad del modelo de aprendizaje automático, ya que pueden dar lugar a una sobreestimación de la importancia de ciertos datos en el modelo y a una subestimación de la variabilidad de los datos. Por lo tanto, es **importante eliminar** los valores duplicados antes de entrenar un modelo para evitar resultados imprecisos o sesgados.\n",
    "\n",
    "Sin embargo, en algunos casos, puede ser apropiado conservar los valores duplicados si su presencia es significativa para el análisis, como en el caso de conjuntos de datos que contienen datos de series temporales o datos de mediciones repetidas en el tiempo. En estos casos, los valores duplicados pueden proporcionar información valiosa sobre patrones de comportamiento y cambios en el tiempo.\n",
    "\n",
    "~~~\n",
    "df_sin_duplicados = df.drop_duplicates()\n",
    "\n",
    "'subset' si solo queremos eliminar los nulos de esas columnas:\n",
    "df_sin_duplicados = df.drop_duplicates(subset=['A', 'B'])\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c63c87",
   "metadata": {},
   "source": [
    "### Valores nulos.\n",
    "\n",
    "Los casos en los que se deben eliminar los datos nulos, depende del contexto del análisis de datos. En general, es importante eliminar los datos nulos para **evitar distorsiones** en los resultados de los análisis y para asegurarse de que los **modelos** de aprendizaje automático se entrenen con **datos completos y precisos**. \n",
    "\n",
    "Sin embargo, en algunos casos, puede ser útil **conservar los datos nulos** si su presencia es significativa para el análisis, como en el caso de datos faltantes que pueden **indicar un patrón de comportamiento específico**.\n",
    "\n",
    "detección de los mismos df.isna().sum() y del relleno de los mismos con fillna()\n",
    "~~~\n",
    "df_sin_nulos = df.dropna()\n",
    "\n",
    "'subset' si solo queremos eliminar los nulos de esas columnas:\n",
    "df_sin_nulos = df.dropna(subset=['A', 'B'])\n",
    "\n",
    "df.isna().sum() y del relleno de los mismos con fillna()\n",
    "~~~\n",
    "\n",
    "**NOTA: NORMALMENTE SE ELIMINAN LOS VALORES NULOS, Y SI SON MUCHOS, SE NECESITAN MAS DATOS.** \n",
    "\n",
    "                                        \"MAQUILLAR\" LOS DATOS, NO ES BUENA PRACTICA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267695b2",
   "metadata": {},
   "source": [
    "## **Agrupación de Variables Categóricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4bf0a",
   "metadata": {},
   "source": [
    "La agrupación de variables categóricas puede ser necesaria para simplificar y mejorar la interpretación de un modelo, mejorar la capacidad del modelo para capturar el efecto de ciertas categorías y reducir la dimensionalidad del modelo. Sin embargo, es importante tener cuidado al agrupar variables categóricas, ya que esto puede llevar a la pérdida de información importante si no se realiza correctamente.\n",
    "\n",
    "- **Cuando las categorías son muy numerosas:** Si hay muchas categorías diferentes en una variable categórica, puede ser difícil para un modelo manejar todas ellas. Agruparlas en categorías más amplias puede simplificar el modelo y hacerlo más fácil de interpretar.\n",
    "\n",
    "\n",
    "- **Cuando algunas categorías tienen muy pocos datos:** Si hay categorías con muy pocos datos, es posible que el modelo no pueda capturar adecuadamente su efecto en el resultado. En este caso, agruparlas con otras categorías puede ayudar a aumentar la cantidad de datos para cada categoría y mejorar la capacidad del modelo para capturar su efecto.\n",
    "\n",
    "\n",
    "- **Cuando hay categorías muy similares entre sí:** Si hay categorías que son muy similares entre sí y que representan la misma idea o concepto, agruparlas en una sola categoría puede hacer que el modelo sea más simple y fácil de interpretar.\n",
    "\n",
    "\n",
    "- **Cuando se quiere reducir la dimensionalidad:** Si hay muchas variables categóricas diferentes en un modelo, puede ser necesario reducir la dimensionalidad para que el modelo no se vuelva demasiado complejo. La agrupación de variables categóricas similares puede ser una forma efectiva de reducir la cantidad de variables y simplificar el modelo.\n",
    "\n",
    "~~~~\n",
    "                    lista=[...] # lista de valores a sustituir\n",
    "                    lista2=[...]\n",
    "                    df['column'] = df['column'].apply(lambda x: 'A' if x in lista else 'B' if x in lista2 else...)\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8961607",
   "metadata": {},
   "source": [
    "## **Codificación - Discretización:** \n",
    "\n",
    "<img src=https://electronicaengeneral.files.wordpress.com/2014/11/muestreo.jpg width=\"400\">\n",
    "\n",
    "- **Codificación:** Transformación de variables categóricas en variables numéricas que se puedan utilizar en el modelo.\n",
    "\n",
    "\n",
    "- **Discretización:** Transformación de variables numericas en variables categoricas que se puedan utilizar en el modelo.\n",
    "\n",
    "La decisión de transformar variables categóricas en numéricas o viceversa depende del contexto del problema y del tipo de análisis que se desea realizar.\n",
    "\n",
    "En general, se transforman **variables categóricas en numéricas** cuando se desea realizar un **análisis estadístico** que requiere datos numéricos, como la regresión lineal o el análisis de varianza. Sin embargo, es importante asegurarse de que la variable categórica tenga una escala de medida ordinal o de intervalo para que la conversión tenga sentido.\n",
    "\n",
    "Por otro lado, se transforman **variables numéricas en categóricas** cuando se desea analizar la **distribución de datos** en grupos discretos, como en el caso de análisis de segmentación de clientes o de agrupamiento de productos. En este caso, la variable numérica se divide en categorías o intervalos, por ejemplo, por rango de edad o por niveles de ingresos.\n",
    "\n",
    "**Cambiar todas las variables Object, Str y Bool** a categóricas en un dataframe en Python puede ser una **buena práctica** para algunos modelos de machine learning, ya que los algoritmos suelen trabajar mejor con variables numéricas y categóricas en lugar de texto o booleanos. Sin embargo, esto no siempre es necesario y puede depender del modelo específico que se esté utilizando y de los datos que se estén analizando.\n",
    "\n",
    "~~~\n",
    "df['mi_variable'] = df['mi_variable'].astype('category')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91a5c4",
   "metadata": {},
   "source": [
    "## **Reducción de dimensionalidad:**\n",
    "\n",
    "<img src=https://aukera.es/blog/imagenes/creaci%C3%B3n-de-dimension.png widht=\"400\">\n",
    "\n",
    "La reducción de dimensionalidad implica la **transformación de los datos de alta dimensionalidad** a un espacio de menor dimensión, mientras se trata de preservar la mayor cantidad posible de información importante. Esta técnica se utiliza para simplificar la representación de datos y mejorar el rendimiento de los algoritmos de aprendizaje automático.\n",
    "\n",
    "Los datos de alta dimensionalidad son conjuntos de **datos que contienen una gran cantidad de características o atributos**. En otras palabras, son conjuntos de datos que tienen un gran número de variables o dimensiones.\n",
    "\n",
    "- **Por ejemplo** una imagen en alta resolución puede tener millones de píxeles y, por lo tanto, puede considerarse como un conjunto de datos de alta dimensionalidad. Otro ejemplo puede ser una base de datos de clientes de una tienda en línea que tiene muchos atributos diferentes para cada cliente, como edad, género, historial de compras, preferencias, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7a1a9",
   "metadata": {},
   "source": [
    "\n",
    "Transformación de variables para que cumplan con los supuestos de normalidad y homocedasticidad, como la transformación de Box-Cox o la transformación logarítmica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce8c6d",
   "metadata": {},
   "source": [
    "## **Clasificación de los Datos**\n",
    "Se divide el Dataframe en subdataframe para posteriores análisis.\n",
    "\n",
    "~~~~\n",
    "          - Variable Objetivo\n",
    "            df_target=df_nout['TARGET'].copy()\n",
    "\n",
    "          - Variables Independientes\n",
    "            df_indepent=df_nout.drop('TARGET', axis=1).copy()\n",
    "\n",
    "          - Variables Independientes Categoricas y Numericas\n",
    "            df_ind_num = df_indepent.select_dtypes(include=[\"int64\"]).copy() (VARIABLES INDEPENDIENTES NUMERICAS)\n",
    "            df_ind_cat= df_indepent.select_dtypes(include=[\"object\"]).copy() (VARIABLES INDEPENDIENTES CATEGORICAS)\n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfc654",
   "metadata": {},
   "source": [
    "## **Interacción entre variables** (var. independientes *(num, cat y num+cat)* y var. independientes+dependiente)\n",
    "\n",
    "<img src=https://multimedia.elsevier.es/PublicationsMultimediaV1/item/multimedia/thumbnail/13099413:4v26n02-13099413tab01.gif width=\"400\">\n",
    "\n",
    "La interacción entre variables en la creación de un modelo se refiere a la **relación entre dos o más variables** que afecta la salida o la variable dependiente que se está tratando de predecir en el modelo. En otras palabras, la interacción entre variables implica que el efecto de una variable en la variable de salida puede **depender de los valores** de otras variables.\n",
    "\n",
    "- **Por ejemplo** en un modelo de predicción de precios de viviendas, la interacción entre la ubicación de la vivienda y el tamaño del jardín podría ser importante. Es decir, el efecto de la ubicación en el precio de la vivienda puede depender del tamaño del jardín. Es posible que en algunas ubicaciones, un jardín grande aumente el precio de la vivienda más que en otras ubicaciones.\n",
    "\n",
    "Es importante considerar la interacción entre variables al crear un modelo, ya que puede mejorar la precisión de las predicciones.\n",
    "\n",
    "**Interacción entre Variables**\n",
    "\n",
    "- **Análisis de correlación:** Se puede examinar la correlación entre pares de variables y también la correlación parcial para detectar la presencia de una interacción.\n",
    "\n",
    "                        corr_matrix = data.corr()\n",
    "                        sns.heatmap(data)\n",
    "\n",
    "- **Análisis gráfico:** La visualización de los datos en gráficos puede ayudar a identificar patrones y relaciones complejas entre variables, incluyendo la interacción. Por ejemplo, se pueden construir diagramas de dispersión para analizar la relación entre dos variables, o gráficos de superficie para analizar la relación tridimensional entre tres variables.\n",
    "\n",
    "                      - Diagrama de Dispersión (2 - Variables)\n",
    "                        plt.scatter(data['variable1'], data['variable2'])\n",
    "\n",
    "                      - Gráfico de superficie (3 - Variables)\n",
    "                        fig = plt.figure()\n",
    "                        ax = fig.add_subplot(111, projection='3d')\n",
    "                        ax.plot_trisurf(data['variable1'], data['variable2'], data['variable3'])\n",
    "\n",
    "\n",
    "- **Análisis de regresión:** Se pueden incluir términos de interacción en los modelos de regresión para analizar si los coeficientes de interacción son significativos.\n",
    "\n",
    "                      - Términos de interacción\n",
    "                        X_interaction = X.copy()\n",
    "                        X_interaction['variable1*variable2'] = X_interaction['variable1'] * X_interaction['variable2']\n",
    "                        model_interaction = LinearRegression()\n",
    "                        model_interaction.fit(X_interaction, y)\n",
    "\n",
    "- **Análisis de varianza (ANOVA):** Se puede realizar un análisis ANOVA para comparar la varianza explicada por un modelo con y sin términos de interacción.\n",
    "\n",
    "                        model_without_interaction = ols('target ~ variable1 + variable2 + variable3', data=data).fit()\n",
    "                        model_with_interaction = ols('target ~ variable1*variable2 + variable3', data=data).fit()\n",
    "\n",
    "                        anova_table = sm.stats.anova_lm(model_without_interaction, model_with_interaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af91450",
   "metadata": {},
   "source": [
    "## **Balanceo de clases:** \n",
    "El balanceo de clases se refiere al proceso de **ajustar la distribución** de las clases de una variable objetivo en un conjunto de datos, para que **NO haya una clase dominante** que pueda afectar negativamente el rendimiento del modelo de aprendizaje automático. En muchos casos, los conjuntos de datos pueden estar desequilibrados, lo que significa que hay muchas más observaciones de una clase que de otra.\n",
    "\n",
    "Cuando se entrena un modelo de aprendizaje automático en un conjunto de datos desequilibrado, es probable que el modelo se sesgue hacia la clase dominante y tenga un rendimiento pobre en la clase minoritaria. Para evitar esto, se puede utilizar el balanceo de clases para ajustar la distribución de las clases de una variable objetivo en un conjunto de datos.\n",
    "\n",
    "<img src=https://www.researchgate.net/profile/Tomas-Mateo-Sanguino/publication/354389400/figure/fig4/AS:1065124949405696@1630956985021/Distribucion-de-las-clases-Reposo-y-Vehiculo-inicialmente-a-la-izquierda-y-posterior-al.png width=\"400\">\n",
    "\n",
    "- **Submuestreo (undersampling):** Este método consiste en eliminar aleatoriamente instancias de la clase mayoritaria hasta que se alcance un equilibrio entre las clases. Aunque puede ser efectivo, puede resultar en la pérdida de información valiosa y en la reducción de la precisión del modelo.\n",
    "\n",
    "                                from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "                                rus = RandomUnderSampler(random_state=0)\n",
    "                                X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "- **Sobremuestreo (oversampling):** Este método consiste en crear nuevas instancias de la clase minoritaria mediante la replicación de instancias existentes o la generación de nuevas instancias sintéticas. Aunque puede aumentar la precisión del modelo, también puede generar problemas de sobreajuste y aumentar el tiempo de procesamiento.\n",
    "\n",
    "                                from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "                                ros = RandomOverSampler(random_state=0)\n",
    "                                X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "- **Combinación de submuestreo y sobremuestreo:** Este método implica la combinación de los dos métodos anteriores, donde se submuestrea la clase mayoritaria y se sobremuestrea la clase minoritaria. Puede ser una buena opción cuando el submuestreo o el sobremuestreo por sí solos no son suficientes.\n",
    "\n",
    "                                from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "                                tl = TomekLinks()\n",
    "                                X_resampled, y_resampled = tl.fit_resample(X, y)\n",
    "\n",
    "- **Generación de nuevas muestras (SMOTE):** Este método utiliza una técnica de interpolación para generar nuevas muestras sintéticas de la clase minoritaria, y se basa en los puntos cercanos de la clase minoritaria para generar los nuevos puntos.\n",
    "\n",
    "                                from imblearn.over_sampling import SMOTE\n",
    "\n",
    "                                smote = SMOTE(random_state=0)\n",
    "                                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "- **Cost-sensitive learning:** Este método implica la asignación de diferentes costos a los errores de clasificación para diferentes clases. Los modelos se entrenan con una función de costos personalizada que penaliza los errores de clasificación de la clase minoritaria más que los errores de la clase mayoritaria. Esto puede ser una buena opción cuando el costo de los errores de clasificación es desigual entre las clases.\n",
    "\n",
    "                                from sklearn.svm import SVC\n",
    "                                from sklearn.utils import class_weight\n",
    "\n",
    "                                class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "                                svc = SVC(kernel='linear', class_weight=class_weights)\n",
    "                                svc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "- **Ensemble learning:** Este método utiliza algoritmos de aprendizaje de conjuntos para combinar varios modelos entrenados en diferentes subconjuntos de los datos. Puede ser efectivo para mejorar la precisión de la clasificación y reducir la variabilidad del modelo.\n",
    "\n",
    "                                from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "                                rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                                rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247f6a6",
   "metadata": {},
   "source": [
    "## **Análisis de multicolinealidad:**\n",
    "La multicolinealidad ocurre cuando **dos o más variables predictoras están altamente correlacionadas**, lo que puede afectar la capacidad del modelo para identificar la verdadera relación entre las variables predictoras y la variable de respuesta.\n",
    "- **VIF (Factor de Inflación de la Varianza):** Valor adimensional que se utiliza para medir la multicolinealidad en un modelo de regresión lineal. El VIF mide la proporción de la varianza de una variable predictora que se puede explicar por las otras variables predictoras en el modelo. Un VIF alto indica que la variable predictora está altamente correlacionada con otras variables predictoras y que puede estar introduciendo ruido en el modelo. Un valor de **VIF de 1** indica que **no** hay **multicolinealidad** entre la **variable predictora** y las **demás** variables predictoras. Un valor de VIF mayor que 1 indica que hay una cierta cantidad de multicolinealidad en el modelo. Generalmente, se considera que un valor de **VIF superior a 5 indica una multicolinealidad problemática.**\n",
    "\n",
    "                    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "                  - Seleccionar las variables predictoras\n",
    "                    X_i = data[['Variable_1', 'Variable_2', 'Variable_3'],...]\n",
    "\n",
    "                  - Calcular el factor de inflación de la varianza (VIF) para cada variable predictora\n",
    "                    vif = pd.DataFrame()\n",
    "                    vif[\"VIF Factor\"] = [variance_inflation_factor(X_i.values, i) for i in range(X_i.shape[1])]\n",
    "                    vif[\"Predictor\"] = X_i.columns\n",
    "\n",
    "                  - Imprimir los resultados del análisis de multicolinealidad\n",
    "                    print(vif)\n",
    "\n",
    "**OTROS**\n",
    "\n",
    "                    - Calcula la matriz de correlación\n",
    "                    corr_matrix = np.corrcoef(data, rowvar=False)\n",
    "- **Condición de número:** La condición de número es una medida que mide la relación entre la mayor y la menor varianza propia en una matriz de correlación. Si la condición de número es alta, puede indicar la presencia de multicolinealidad.\n",
    "\n",
    "                    condition_number = np.linalg.cond(corr_matrix)\n",
    "                    print(condition_number)\n",
    "\n",
    "- **Análisis de valores propios:** El análisis de valores propios es una técnica que se utiliza para descomponer una matriz de correlación en sus componentes principales. La multicolinealidad se puede medir observando los valores propios de la matriz de correlación. Si los valores propios son altos, puede indicar la presencia de multicolinealidad.\n",
    "\n",
    "                    eigenvalues, eigenvectors = eig(corr_matrix)\n",
    "                    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n",
    "                    print(sorted_eigenvalues)\n",
    "\n",
    "- **Factor de inflación de la varianza media (VIFM):** El VIFM es similar al VIF, pero en lugar de medir el efecto de una variable predictora sobre otra, mide el efecto combinado de varias variables predictoras sobre otra variable predictora. El VIFM se utiliza a menudo en modelos con múltiples variables de respuesta.\n",
    "\n",
    "                    vifm = np.mean(vif)\n",
    "                    print(vifm)\n",
    "                    \n",
    "- **Y MAS...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dcf19",
   "metadata": {},
   "source": [
    "**DIFERENCIAS ENTRE CORRELACIÓN Y MULTICOLINEALIDAD**\n",
    "\n",
    "- El **análisis de correlación** se utiliza para medir la **relación lineal** entre dos variables, mientras que el **VIF** se utiliza para medir la **cantidad de varianza** de una variable predictora que se puede **explicar por las otras variables predictoras** en el modelo.\n",
    "\n",
    "\n",
    "- En otras palabras, el VIF es una medida específica de la multicolinealidad que se centra en la correlación entre variables predictoras en un modelo de regresión. Por otro lado, el análisis de correlación es una medida más general de la relación lineal entre dos variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c20fd",
   "metadata": {},
   "source": [
    "## **Codificación Variables** (DUMMIES)\n",
    "**Variable Dummy** \n",
    "\n",
    "Son variables binarias que se utilizan en el aprendizaje automático para representar características categóricas. Estas variables toman el valor 1 si una observación pertenece a una determinada categoría y 0 en caso contrario. Se utilizan para convertir las variables categóricas en una forma que puede ser procesada por algoritmos de aprendizaje automático que requieren datos numéricos.\n",
    "\n",
    "                                for col in cat_col:\n",
    "                                    col+_dummy = pd.get_dummies (df[col], prefix=f'{col}')\n",
    "\n",
    "<img src=https://fhernanb.github.io/libro_regresion/images/var_cuali_01.png width=\"600\">\n",
    "\n",
    "\n",
    "**Código:**\n",
    "\n",
    "Para cada columna en la lista de columnas **categóricas**, crea la variable \"nombre_de_la columna\"_dummy que es el codigo binario de la columna.\n",
    "\n",
    "**get_dummies:** \n",
    "\n",
    "Es una función de Pandas que se utiliza para convertir variables categóricas en variables numéricas binarias. Crea columnas adicionales para cada categoría en una columna dada y asigna un valor de 1 o 0 a cada fila según la presencia o ausencia de esa categoría. Esto es útil para el análisis de datos y el modelado predictivo, ya que muchos algoritmos requieren datos numéricos para funcionar.\n",
    "\n",
    "**OneHotEnconder vs get_dummies**\n",
    "\n",
    "Tanto get_dummies como OneHotEncoder se utilizan para convertir variables categóricas en variables numéricas en el preprocesamiento de datos. Sin embargo, hay algunas diferencias clave entre ambas:\n",
    "\n",
    "- **Librería:** get_dummies es una función de pandas, mientras que OneHotEncoder es una clase de la librería scikit-learn.\n",
    "\n",
    "\n",
    "- **Entrada:** get_dummies toma un objeto pandas DataFrame como entrada y convierte automáticamente todas las columnas categóricas en variables ficticias. En cambio, OneHotEncoder requiere que se seleccione manualmente la(s) columna(s) a transformar.\n",
    "\n",
    "\n",
    "- **Salida:** get_dummies devuelve un nuevo objeto DataFrame con las variables ficticias añadidas, mientras que OneHotEncoder devuelve una matriz numpy con las variables numéricas resultantes.\n",
    "\n",
    "\n",
    "- **Flexibilidad:** OneHotEncoder ofrece más opciones de personalización que get_dummies, como la posibilidad de especificar cómo manejar las categorías que no están presentes en los datos de entrenamiento.\n",
    "\n",
    "\n",
    "~~~\n",
    "                                from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "                                - Creamos una instancia de OneHotEncoder y ajustamos los datos\n",
    "                                    encoder = OneHotEncoder()\n",
    "                                    encoder.fit(data)\n",
    "\n",
    "                                - Transformamos los datos en variables ficticias\n",
    "                                    encoded_data = encoder.transform(data).toarray()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141caa2a",
   "metadata": {},
   "source": [
    "## **Selección de características - Componenetes principales :** \n",
    "La selección de características (también conocida como selección de atributos) es un proceso en el aprendizaje automático que implica elegir un **subconjunto de características o variables relevantes** para la construcción de un modelo predictivo. El objetivo de la selección de características es reducir la dimensionalidad de los datos y mejorar la precisión y eficiencia de un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6a27f",
   "metadata": {},
   "source": [
    "## **Train - Test** \n",
    "Consiste en **dividir** el conjunto de datos en dos subconjuntos: uno para entrenar el modelo (datos de entrenamiento - **TRAIN**) y otro para evaluar su rendimiento (datos de prueba - **TEST**).\n",
    "\n",
    "<img src=https://miro.medium.com/v2/resize:fit:720/format:webp/1*4G__SV580CxFj78o9yUXuQ.png width=\"400\">\n",
    "\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "                                - Cargar datos\n",
    "                                  X = ...  # matriz de características\n",
    "                                  y = ...  # vector de etiquetas\n",
    "\n",
    "                                - Dividir datos en conjunto de entrenamiento y conjunto de prueba\n",
    "                                  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                                  \n",
    "**Validation set (conjunto de validación):** Es un conjunto de datos adicional que se utiliza para ajustar los hiperparámetros del modelo. Los hiperparámetros son valores que se establecen antes del entrenamiento del modelo y que afectan el rendimiento del mismo. El conjunto de validación se utiliza para ajustar estos valores y encontrar la configuración óptima de los hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa58c88",
   "metadata": {},
   "source": [
    "## **Transformacion de variables: Estandarización, Escalado...**\n",
    "\n",
    "<img src=https://pro.arcgis.com/es/pro-app/2.9/tool-reference/data-management/GUID-71C48BE3-D4E3-422C-A241-089FE0784469-web.png width=\"600\">\n",
    "\n",
    "**Técnicas para ajustar la escala de los datos**\n",
    "\n",
    "\n",
    "- **Estandarización:** es el proceso de ajustar los valores de una columna para que tengan una media de cero y una desviación estándar de uno. Esto se hace para que los datos estén centrados alrededor de cero y tengan una varianza comparable. La estandarización es útil cuando se conocen las distribuciones de los datos y se desea que los datos se ajusten a una distribución normal.\n",
    "    ~~~\n",
    "                                - Crear una instancia del escalador\n",
    "                                    scaler = StandardScaler()\n",
    "\n",
    "                                - Aplicar el escalado a los datos en X e y\n",
    "                                  data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "                                  nums_train = scaler.fit_transform(X_train[nums])\n",
    "                                  nums_test = scaler.fit_transform(X_test[nums])\n",
    "    ~~~\n",
    "- **Min-Max scaler:** esta técnica transforma los datos para que estén en un rango específico, típicamente de 0 a 1 o -1 a 1. Se logra restando el valor mínimo y dividiendo por la diferencia entre el valor máximo y mínimo.\n",
    "    ~~~\n",
    "                                - Crear una instancia del escalador\n",
    "                                  scaler = MinMaxScaler()\n",
    "    ~~~\n",
    "\n",
    "- **Robust scaler:** esta técnica escala los datos por la mediana y la diferencia intercuartil para evitar el impacto de valores extremos en la distribución de los datos.\n",
    "    ~~~\n",
    "                                - Crear una instancia del escalador\n",
    "                                  scaler = RobustScaler()\n",
    "    ~~~\n",
    "\n",
    "- **MaxAbs scaler:** esta técnica divide cada valor por el valor absoluto máximo para escalar los datos dentro del rango de -1 a 1.\n",
    "    ~~~\n",
    "                                - Crear una instancia del escalador\n",
    "                                  scaler = MaxAbsScaler()\n",
    "    ~~~\n",
    "\n",
    "**Técnicas de transformación de distribución:**\n",
    "- **Transformación Box-Cox:** esta técnica transforma los datos para que sigan una distribución normal. Es útil cuando los datos no siguen una distribución normal y se requiere una transformación para aplicar ciertos modelos estadísticos.\n",
    "~~~\n",
    "                                from scipy.stats import boxcox\n",
    "\n",
    "                                X_boxcox, _ = boxcox(X)\n",
    "~~~\n",
    "- **Yeo-Johnson:** La transformación de Yeo-Johnson es una variante del método Box-Cox que funciona con datos que incluyen valores negativos.\n",
    "~~~\n",
    "                                from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "                                pt = PowerTransformer(method='yeo-johnson')\n",
    "                                X_pt = pt.fit_transform(X)\n",
    "~~~\n",
    "- **Power transform:** La transformación de Power busca una función que haga que los datos tengan una distribución normal. Puede ser útil para modelos que asumen una distribución normal de los datos.\n",
    "~~~\n",
    "                                from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "                                pt = PowerTransformer(method='box-cox')\n",
    "                                X_pt = pt.fit_transform(X)\n",
    "~~~\n",
    "- **Discretización:** La discretización es la transformación de variables numéricas en variables categóricas. Puede ser útil para modelos que requieren variables categóricas o para reducir el ruido en los datos. (**PUNTO 4.2**)\n",
    "\n",
    "\n",
    "- **Quantile transform:** La transformación de quantile mapea los datos a una distribución uniforme y luego a una distribución normal. Puede ser útil para modelos que asumen una distribución normal de los datos.\n",
    "~~~~\n",
    "                                from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "                                qt = QuantileTransformer(n_quantiles=10, output_distribution='normal')\n",
    "                                X_qt = qt.fit_transform(X)\n",
    "~~~~\n",
    "- **Transformación Logarítmica:** esta técnica aplica una transformación logarítmica a los datos. Es útil cuando los datos tienen una distribución sesgada a la derecha y se quiere reducir el sesgo.\n",
    "~~~\n",
    "                                import numpy as np\n",
    "\n",
    "                                X_log = np.log(X)\n",
    "~~~\n",
    "- **Raíz cuadrada:** La transformación de la raíz cuadrada se utiliza a menudo para reducir la varianza de los datos. Puede ser útil cuando los datos tienen una distribución sesgada hacia valores bajos.\n",
    "\n",
    "~~~\n",
    "                                X_sqrt = np.sqrt(X)\n",
    "~~~\n",
    "- **Transformación inversa:** Esta técnica se utiliza para transformar valores que han sido transformados previamente a su escala original.\n",
    "~~~\n",
    "                                X_orig = scaler.inverse_transform(X_scaled)\n",
    "~~~\n",
    "\n",
    "**Técnica para ajustar los límites de los datos**\n",
    "El truncamiento implica establecer un límite superior o inferior en los valores de la variable. Todos los valores por encima o por debajo del límite se establecen en el valor límite. Esta técnica puede ser útil cuando se sabe que ciertos valores son imposibles o son errores de medición.\n",
    "~~~~\n",
    "                                import numpy as np\n",
    "\n",
    "                                - Creamos un array de ejemplo\n",
    "                                x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "                                - Establecemos el límite inferior y superior\n",
    "                                lim_inf = 2\n",
    "                                lim_sup = 8\n",
    "\n",
    "                                - Aplicamos el truncamiento\n",
    "                                x_truncado = np.clip(x, lim_inf, lim_sup)\n",
    "\n",
    "~~~~\n",
    "\n",
    "El binning implica agrupar los valores de la variable en un número finito de categorías o \"bins\". Esta técnica puede ser útil cuando se tienen datos continuos pero se desea trabajar con datos categóricos o discretos. Por ejemplo, se puede utilizar para crear rangos de edades o ingresos.\n",
    "~~~~\n",
    "                                from sklearn.preprocessing import KBinsDiscretizer\n",
    "                                import numpy as np\n",
    "\n",
    "                                - Creamos un array de ejemplo\n",
    "                                x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "                                - Establecemos el número de bins\n",
    "                                n_bins = 5\n",
    "\n",
    "                                - Creamos un objeto KBinsDiscretizer y lo ajustamos a los datos\n",
    "                                kb = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "                                kb.fit(x.reshape(-1, 1))\n",
    "\n",
    "                                - Aplicamos el binning\n",
    "                                x_binning = kb.transform(x.reshape(-1, 1)).flatten()\n",
    "~~~~\n",
    "\n",
    "**Y MÁS...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e6086",
   "metadata": {},
   "source": [
    "# **Tipos: Distribuciones** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd06b5",
   "metadata": {},
   "source": [
    "Un **test de uniformidad** es una prueba estadística que se utiliza para determinar si los datos se distribuyen de manera uniforme en un **rango determinado.**\n",
    "\n",
    "Un **test de bondad** es una prueba estadística que se utiliza para determinar si un conjunto de datos sigue una **distribución específica.**\n",
    "\n",
    "Un **test de normalidad** es una prueba estadística que se utiliza para determinar si un conjunto de datos sigue una **distribución normal o gaussiana.**\n",
    "\n",
    "\n",
    "- **Distribución normal:** \n",
    "\n",
    "<img src=https://economipedia.com/wp-content/uploads/Captura-de-pantalla-2019-09-10-a-les-11.09.35.png width=\"400\">\n",
    "\n",
    "        - Test de normalidad de Shapiro-Wilk.\n",
    "        - Test de normalidad de Anderson-Darling.\n",
    "        - Test de normalidad de Kolmogorov-Smirnov. \n",
    "\n",
    "- **Distribución bimodal:** \n",
    "\n",
    "<img src=https://www.researchgate.net/publication/363774562/figure/fig1/AS:11431281085885730@1663938284472/Figura-1a-Distribucion-simetrica-bimodal-con-las-modas-en-valores-intermedios-de-la.ppm width=\"400\">\n",
    "\n",
    "        - Test de normalidad de Anderson-Darling.\n",
    "        - Test de normalidad de Kolmogorov-Smirnov.\n",
    "\n",
    "- **Distribución uniforme:** \n",
    "\n",
    "<img  src=https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Uniform_distribution_PDF.png/1200px-Uniform_distribution_PDF.png width=\"400\">\n",
    "\n",
    "        - Test de uniformidad de Chi-Cuadrado.\n",
    "\n",
    "- **Distribución de Poisson:** \n",
    "\n",
    "<img src=https://www.lifeder.com/wp-content/uploads/2019/09/poisson1.jpg width=\"400\">\n",
    "\n",
    "        - Test de normalidad de Anderson-Darling.\n",
    "        - Test de normalidad de Kolmogorov-Smirnov Test de goodness of fit de Poisson.\n",
    "        \n",
    "- **Distribución exponencial:**\n",
    "\n",
    "<img src= https://miro.medium.com/max/778/1*TZvxiHi8loOjSvvGYT48-A.png width=\"400\">\n",
    "\n",
    "        - Test de normalidad de Anderson-Darling.\n",
    "        - Test de normalidad de Kolmogorov-Smirnov.\n",
    "        - Test de bondad de ajuste de Kolmogorov-Smirnov para distribución exponencial.\n",
    "\n",
    "- **Distribución de Bernoulli:**\n",
    "\n",
    "<img src=http://www5.uva.es/estadmed/probvar/d_univar/bernoulli.gif width=\"400\">\n",
    "\n",
    "        - Test de bondad de ajuste de Chi-Cuadrado.\n",
    "        - Test exacto de Fisher.\n",
    "\n",
    "- **Distribución binomial:**\n",
    "\n",
    "<img src=https://fisicaymates.com/wp-content/uploads/2016/04/binomial-distribution-critical-value.jpg width=\"400\">\n",
    "\n",
    "        - Test de bondad de ajuste de Chi-Cuadrado.\n",
    "        - Test exacto de Fisher.\n",
    "        - Test de bondad de ajuste de Kolmogorov-Smirnov para distribución binomial.\n",
    "\n",
    "- **Distribución de chi-cuadrado:**\n",
    "\n",
    "<img src=https://www.lifeder.com/wp-content/uploads/2020/08/chi-cuadrado-3.jpg width=\"400\">\n",
    "\n",
    "        - Test de bondad de ajuste de Chi-Cuadrado.\n",
    "        - Test de normalidad de Anderson-Darling.\n",
    "        - Test de normalidad de Kolmogorov-Smirnov.\n",
    "\n",
    "- **Distribución t de Student:**\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/c/cf/Student_densite_best.JPG width=\"400\">\n",
    "\n",
    "        - Test de normalidad de Anderson-Darling.\n",
    "        - Test de normalidad de Kolmogorov-Smirnov.\n",
    "        - Test t de Student.\n",
    "        \n",
    "- **Y MÁS...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e2e1d",
   "metadata": {},
   "source": [
    "## Ejemplos de Test: Supuestos y Limitaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c11ce7",
   "metadata": {},
   "source": [
    "- **Test de normalidad de Shapiro-Wilk:** \n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos. \n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas (menos de 50 observaciones) o para distribuciones muy no normales.\n",
    "\n",
    "\n",
    "- **Test de normalidad de Anderson-Darling:** \n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos. \n",
    "    - **Limitaciones:** puede ser más sensible a outliers que otros test de normalidad.\n",
    "\n",
    "\n",
    "- **Test de normalidad de Kolmogorov-Smirnov:** \n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos. \n",
    "    - **Limitaciones:** puede ser menos preciso para distribuciones muy no normales.\n",
    "\n",
    "\n",
    "- **Test de uniformidad de Chi-Cuadrado:** \n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y se deben contar en intervalos discretos.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas.\n",
    "\n",
    "\n",
    "- **Test de goodness of fit de Poisson:** \n",
    "    - **Supuestos:** los datos deben seguir una distribución de Poisson y deben ser independientes e identicamente distribuidos. \n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones que están lejos de ser perfectamente Poisson\n",
    "   \n",
    "   \n",
    "- **Test de bondad de ajuste de Lilliefors:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y se supone que los datos provienen de una distribución normal.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas y puede ser engañoso para distribuciones muy no normales.\n",
    "\n",
    "\n",
    "- **Test de hipótesis de K-S para dos muestras:**\n",
    "    - **Supuestos:** las dos muestras deben ser independientes e identicamente distribuidas.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy no similares.\n",
    "\n",
    "\n",
    "- **Test de homogeneidad de varianza de Levene:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas y puede ser influenciado por outliers.\n",
    "\n",
    "\n",
    "- **Test de homogeneidad de proporciones de Z:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben contarse en categorías.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones con una gran cantidad de categorías.\n",
    "\n",
    "\n",
    "- **Test de varianza de Bartlett:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de varias poblaciones normales.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones con una gran cantidad de variabilidad entre las diferentes poblaciones.\n",
    "\n",
    "\n",
    "- **Test de bondad de ajuste de Cramér-von Mises:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de una distribución continua.\n",
    "    - **Limitaciones**: puede ser menos preciso para muestras pequeñas o para distribuciones muy no similares a la distribución supuesta.\n",
    "    \n",
    "\n",
    "- **Test de correlación de Pearson:**\n",
    "    - **Supuestos:** los datos deben ser continuos y linealmente correlacionados.\n",
    "    - **Limitaciones:** no es adecuado para datos categóricos o no linealmente correlacionados.\n",
    "    \n",
    "    \n",
    "- **Test de correlación de Spearman:**\n",
    "    - **Supuestos:** los datos deben ser ordinales o continuos y estar correlacionados de manera monótona.\n",
    "    - **Limitaciones:** puede ser menos preciso para datos muy dispersos o con outliers.\n",
    "    \n",
    "    \n",
    "- **Test de independencia de t de Student:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos poblaciones normales.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy no normales.\n",
    "    \n",
    "    \n",
    "- **Test de diferencia de medias de Welch:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos poblaciones con varianzas diferentes.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy no similares.\n",
    "\n",
    "\n",
    "- **Test de varianzas de Levene:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos o más poblaciones con varianzas posiblemente diferentes.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy no similares.\n",
    "\n",
    "\n",
    "- **Test de hipótesis de Mann-Whitney U:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos poblaciones con distribuciones no necesariamente normales.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy similares.\n",
    "\n",
    "\n",
    "- **Test de Bondad de Ajuste de Lilliefors:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de una distribución normal o similar a una normal.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy no similares a la distribución normal.\n",
    "\n",
    "\n",
    "- **Test de Hipótesis de Wilcoxon Rank-Sum:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos poblaciones con distribuciones no necesariamente normales.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy similares.\n",
    "\n",
    "\n",
    "- **Test de homogeneidad de varianzas de Bartlett:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos o más poblaciones con varianzas posiblemente diferentes.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy no similares.\n",
    "\n",
    "\n",
    "- **Test de hipótesis de Kruskal-Wallis:**\n",
    "    - **Supuestos:** los datos deben ser independientes e identicamente distribuidos y deben provenir de dos o más poblaciones con distribuciones no necesariamente normales.\n",
    "    - **Limitaciones:** puede ser menos preciso para muestras pequeñas o para distribuciones muy similares.\n",
    "    \n",
    "\n",
    "- **Y MÁS...**\n",
    "\n",
    "\n",
    "**Bibliografia**\n",
    "Libro: \"An Introduction to Statistical Learning\" de Gareth James, Daniela Witten, Trevor Hastie y Robert Tibshirani\n",
    "Libro: \"Discovering Statistics Using R\" de Andy Field Libro: \"Statistics for People Who (Think They) Hate Statistics\" de Neil J. Salkind Libro: \"Applied Multivariate Statistical Analysis\" de Richard Johnson y Dean Wichern Página web: scipy.org Página web: statsmodels.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8867b",
   "metadata": {},
   "source": [
    "# **Datos Representativos** ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92c43b",
   "metadata": {},
   "source": [
    " Si los residuos siguen una distribución aleatoria y no muestran patrones sistemáticos, entonces es probable que el modelo sea representativo de los datos. Por otro lado, si los residuos muestran patrones sistemáticos, como una tendencia creciente o decreciente en el tiempo o una relación no lineal entre la variable objetivo y las variables predictoras, entonces el modelo podría no ser representativo de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2d3d2",
   "metadata": {},
   "source": [
    " # **Modelos**:\n",
    " **Variables de Regresión**\n",
    " - **y(x)** o **y:** Variable Dependiente o Respuesta\n",
    " \n",
    " \n",
    " - **X** o **X_i:** Variable/s Independiente/s, Predictora o Explicativas\n",
    " \n",
    " \n",
    " - **βo:** Intercepto \n",
    " \n",
    " \n",
    " - **βn:** Coeficientes de Regresión\n",
    " \n",
    " \n",
    " - **ε:** Error Residual\n",
    " \n",
    " \n",
    "**Métodos más comunes utilizados en los modelos de machine learning:**\n",
    "\n",
    "- **.fit():** Método utilizado para ajustar el modelo a los datos de entrenamiento. En este método, se estima los parámetros del modelo que mejor ajustan los datos de entrenamiento.\n",
    "\n",
    "\n",
    "- **.predict():** Método utilizado para predecir las etiquetas de clase o valores de respuesta para nuevos datos.\n",
    "\n",
    "\n",
    "- **.transform():** Método utilizado para transformar los datos, como ajustar la escala o codificar variables categóricas.\n",
    "\n",
    "\n",
    "- **.score():** Método utilizado para evaluar el desempeño del modelo utilizando un conjunto de datos de prueba.\n",
    "\n",
    "\n",
    "- **.get_params():** Método utilizado para obtener los parámetros del modelo.\n",
    "\n",
    "\n",
    "- **.set_params():** Método utilizado para establecer los valores de los parámetros del modelo.\n",
    "\n",
    "\n",
    "- **.cross_val_score():** Método utilizado para realizar una validación cruzada del modelo, evaluando el desempeño en múltiples divisiones del conjunto de datos de entrenamiento.\n",
    "\n",
    "\n",
    "- **.predict_proba():** Método utilizado para obtener la probabilidad de pertenecer a cada clase en modelos de clasificación.\n",
    "\n",
    "\n",
    "- **.coef_:** Atributo utilizado para obtener los coeficientes del modelo de regresión.\n",
    "\n",
    "\n",
    "- **.intercept_:** Atributo utilizado para obtener el término independiente del modelo de regresión.\n",
    "\n",
    "- **.fit_transform():** Método utilizado para ajustar y transformar simultáneamente los datos de entrenamiento.\n",
    "\n",
    "\n",
    "- **.score_samples():** Método utilizado para calcular la puntuación de probabilidad de los datos de entrada en modelos de clustering basados en densidad.\n",
    "\n",
    "\n",
    "- **.transform_inverse():** Método utilizado para revertir la transformación de los datos de entrada que se realizó previamente utilizando el método .transform().\n",
    "\n",
    "\n",
    "- **.decision_function():** Método utilizado para obtener la función de decisión en modelos de clasificación lineal.\n",
    "\n",
    "\n",
    "- **.classes_:** Atributo utilizado para obtener las etiquetas de clase de un modelo de clasificación.\n",
    "\n",
    "\n",
    "- **.n_features_:** Atributo utilizado para obtener el número de características o variables en un modelo de aprendizaje automático.\n",
    "\n",
    "\n",
    "- **.n_outputs_:** Atributo utilizado para obtener el número de salidas en un modelo de aprendizaje automático con múltiples salidas.\n",
    "\n",
    "\n",
    "- **.get_support():** Método utilizado para obtener una máscara booleana que indica las características seleccionadas por un modelo de selección de características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ee9a6",
   "metadata": {},
   "source": [
    "## **Modelos Supervisados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1673c",
   "metadata": {},
   "source": [
    "### **Modelos Paramétricos:**\n",
    "Es un modelo que utiliza un **conjunto finito de parámetros** para describir la distribución subyacente de los datos. Los modelos paramétricos **asumen una distribución** específica para los datos y **utilizan estimaciones de los parámetros** de esta distribución para hacer predicciones.\n",
    "\n",
    "Algunos modelos paramétricos:\n",
    "\n",
    "**BÁSICOS**\n",
    "1. **Regresión lineal**\n",
    "1. **Regresión Multilineal**\n",
    "1. **Regresión logística**\n",
    "**OTROS:**\n",
    "1. Análisis de varianza (ANOVA)\n",
    "1. Análisis de covarianza (ANCOVA)\n",
    "1. Modelos de regresión polinómica\n",
    "1. Análisis de regresión por pasos (Stepwise regression)\n",
    "1. Análisis discriminante lineal\n",
    "1. Análisis de la varianza multivariante (MANOVA)\n",
    "1. Análisis de la covarianza multivariante (MANCOVA)\n",
    "1. Modelos de series de tiempo autoregresivos (AR)\n",
    "1. Modelos de series de tiempo de media móvil (MA)\n",
    "1. Modelos de series de tiempo autoregresivos de media móvil (ARMA)\n",
    "1. Modelos de series de tiempo autoregresivos integrados de media móvil (ARIMA)\n",
    "1. Modelos de series de tiempo estacionales (Seasonal ARIMA)\n",
    "1. Modelos de regresión de Poisson\n",
    "1. Modelos de regresión binomial negativa\n",
    "1. Modelos de efectos aleatorios\n",
    "1. Modelos de efectos mixtos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927585e",
   "metadata": {},
   "source": [
    "#### Regresion Lineal:\n",
    "\n",
    "En estadística, la regresión lineal o ajuste lineal es un modelo matemático usado para aproximar la relación de dependencia entre una variable dependiente **y**, e una variable independiente **X1**.\n",
    "\n",
    "                                        y(x) = βo + β1·X1 + ε\n",
    "\n",
    "Este método es aplicable en muchas situaciones en las que se estudia la relación entre **dos o más variables o predecir un comportamiento**, algunas incluso sin relación con la tecnología. En caso de que **no se puede** aplicar un modelo de regresión a un estudio, se dice que **no hay correlación** entre las variables estudiadas.\n",
    "\n",
    "<img src=https://economipedia.com/wp-content/uploads/Regresi%C3%B3n-lineal.png width=\"400\">>\n",
    "\n",
    "- **OLS, o regresión lineal ordinaria:** es un método de regresión que se utiliza para ajustar una línea recta a un conjunto de datos. OLS minimiza la suma de los errores al cuadrado entre los valores observados y los valores predichos por el modelo.\n",
    "\n",
    "                                \n",
    "                                        import statsmodels.api as sm\n",
    "                                        modelo_lineal_OLS = sm.OLS(y_train, X_train)\n",
    "                               \n",
    "\n",
    "\n",
    "- **Linear Regression** es un enfoque más general que se refiere a cualquier modelo de regresión que asume una relación lineal entre la variable dependiente y las variables independientes. Linear Regression no se limita a ajustar una línea recta a los datos, sino que también puede ajustar modelos polinómicos o modelos de regresión lineal múltiple.\n",
    "\n",
    "                               \n",
    "                                        from sklearn.linear_model import LinearRegression\n",
    "                                        modelo_lineal= LinearRegression(fit_intercept=True)\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b4429",
   "metadata": {},
   "source": [
    "#### Regresion Lineal Multiple:\n",
    "\n",
    "La regresión lineal múltiple es una técnica estadística utilizada para examinar la relación entre una variable dependiente **y**\n",
    "y dos o más variables independientes **X_i** (también conocidas como variables explicativas o predictoras) continuas.\n",
    "\n",
    "La regresión lineal múltiple trata de encontrar una ecuación lineal que pueda predecir el valor de la variable de respuesta en función de los valores de las variables explicativas, minimizando el error residual.\n",
    "\n",
    "                                        y(x) = βo·Xo + β1·X1 + β2·X2 ...βn·Xn + ε\n",
    "\n",
    "Donde **y** es la variable de respuesta,**X1**, **X2**, ..., **Xn** son las variables explicativas, **β0** es la intercepción, **β1, β2, ..., βn** son los coeficientes de regresión que indican la relación entre cada variable explicativa y la variable de respuesta, y ε es el error residual.\n",
    "\n",
    "<img src=https://www.researchgate.net/publication/311548169/figure/fig4/AS:667625285763081@1536185680916/Figura-6-Modelo-de-regresion-lineal-con-dos-regresores.png width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a591db9",
   "metadata": {},
   "source": [
    "#### Regresiones Logística (Clasificación):\n",
    "La Regresión Logística es un algoritmo de clasificación que se utiliza para **predecir** la probabilidad de una **variable dependiente categórica**. En la regresión logística, la variable dependiente es una variable binaria que contiene datos codificados como 1 – 0, sí – no, abierto – cerrado, etc.\n",
    "\n",
    "<img src=https://conceptosclaros.com/wp-content/uploads/2019/01/curva-logistica-wikipedia.jpg width=\"400\">\n",
    "\n",
    "El resultado o variable objetivo es de naturaleza dicotómica. Dicotómica significa que solo hay **dos clases posible**s. Por ejemplo, se puede utilizar para problemas de detección de cáncer o calcular la probabilidad de que ocurra un evento.\n",
    "\n",
    "<img src=https://www.juanbarrios.com/wp-content/uploads/2019/07/MATRIZ-CONFUSION-400x358.png width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f5f91",
   "metadata": {},
   "source": [
    "~~~\n",
    "                                      - Instanciar el modelo:\n",
    "                                        logisticRegr = LogisticRegression(max_iter=10000)\n",
    "                                        # entrenamiento\n",
    "                                        logisticRegr.fit(X_train, y_train)\n",
    "                                        \n",
    "                                      - Testeo del modelo\n",
    "                                        y_pred = logisticRegr.predict(X_test)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a1247d",
   "metadata": {},
   "source": [
    "## **Modelos No-Paramétricos**\n",
    "Un modelo no paramétrico es un tipo de modelo que **no hace suposiciones** explícitas sobre la forma de la **distribución** subyacente de los **dato**s. A diferencia de los modelos paramétricos, los modelos no paramétricos **no requieren** que se especifique una **función de probabilidad** o una **distribución específica**.\n",
    "\n",
    "En cambio, los modelos no paramétricos se basan en algoritmos y técnicas de aprendizaje automático que utilizan los datos para aprender la estructura subyacente de los mismos. Estos modelos **son útiles** cuando **no** se tiene conocimiento previo de la **distribución de los datos o cuando la forma de la distribución es muy compleja** para ser descrita por una función de probabilidad simple.\n",
    "\n",
    "**Algunos Modelos:**\n",
    "\n",
    "1. **Árboles de decisión:** Estos modelos dividen los datos en subconjuntos cada vez más pequeños utilizando reglas de decisión basadas en las características de los datos.\n",
    "\n",
    "\n",
    "2. **K vecinos más cercanos (KNN):** Este modelo clasifica nuevos puntos de datos según la clase a la que pertenecen los puntos de datos más cercanos.\n",
    "\n",
    "\n",
    "3. **Redes neuronales artificiales:** Estos modelos imitan el funcionamiento del cerebro humano mediante la conexión de neuronas artificiales y son utilizados para problemas de clasificación y regresión.\n",
    "\n",
    "\n",
    "4. **Máquinas de vectores de soporte (SVM):** Estos modelos se utilizan para la clasificación y regresión y encuentran la mejor separación entre las clases al maximizar la distancia entre las clases.\n",
    "\n",
    "\n",
    "5. **Métodos de clustering:** Estos modelos agrupan los datos en grupos o clústeres basados en similitudes entre los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95658828",
   "metadata": {},
   "source": [
    "###  Modelo KNN\n",
    "Es un modelo de aprendizaje supervisado que se basa en la **cercanía de los puntos de datos** en el espacio de características para clasificar o **predecir** nuevos puntos de datos. Se asume que los puntos de datos similares se agrupan en la misma clase o tienen valores de respuesta similares.\n",
    "\n",
    "En KNN, se busca **clasificar o predecir** el valor de un punto de datos desconocido **utilizando la información de los puntos de datos más cercanos** en el espacio de características\n",
    "\n",
    "<img src=https://helloacm.com/wp-content/uploads/2016/03/2012-10-26-knn-concept.png width=\"400\">\n",
    "\n",
    "~~~~\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "- Crear un objeto de clasificador KNN con k=3 (nº vecinos)\n",
    "  knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "- Ajustar el clasificador a los datos de entrenamiento\n",
    "  knn.fit(X_train, y_train)\n",
    "\n",
    "- Clasificar un nuevo punto de datos\n",
    "  X_new = []\n",
    "  y_pred = knn.predict(X_new)\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891e516",
   "metadata": {},
   "source": [
    "### Arbol de Decisión (Decision Tree)\n",
    "Un modelo de árbol de decisión es un modelo predictivo que se construye a partir de un conjunto de datos de entrenamiento, en el que cada nodo interno representa una característica del conjunto de datos y cada hoja representa una clase de respuesta. El objetivo es maximizar la ganancia de información (o reducción en la entropía) en cada división, lo que significa que el árbol se divide de tal manera que la cantidad de incertidumbre o desorden en el conjunto de datos se reduce tanto como sea posible.\n",
    "\n",
    "El modelo se utiliza para hacer predicciones sobre nuevos datos al clasificarlos según la ruta tomada desde la raíz hasta la hoja correspondiente. Los modelos de árbol de decisión son populares debido a su facilidad de interpretación y capacidad para manejar una mezcla de tipos de datos, pero también pueden sufrir de sobreajuste si se construyen árboles demasiado grandes y complicados.\n",
    "\n",
    "<img src=https://www.grupodabia.com/post/2020-05-19-arbol-de-decision/arbol-decision.png width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7554b0",
   "metadata": {},
   "source": [
    "~~~~\n",
    "arbol = DecisionTreeClassifier(criterion='gini', max_depth=depth, min_samples_leaf=1, min_samples_split=2, ccp_alpha=0)\n",
    "arbol.fit(X_train, y_train)\n",
    "~~~~\n",
    "- **criterion:** criterio utilizado para evaluar la calidad de la división de los nodos del árbol. \n",
    "\n",
    "\n",
    "    - \"gini\": para utilizar el índice de impureza de Gini como criterio de división. Es el valor por defecto si no se especifica nada.\n",
    "    \n",
    "\n",
    "    - \"entropy\": para utilizar la entropía como criterio de división.\n",
    "    \n",
    "    \"Ambos criterios miden la homogeneidad de las muestras en un nodo. El criterio de Gini tiende a ser un poco más rápido en términos de tiempo de ejecución, mientras que el criterio de entropía tiende a favorecer las particiones que producen clases más uniformes en los nodos del árbol.\"\n",
    "    \n",
    "\n",
    "- **max_depth:** la profundidad máxima del árbol. Esto limita el número de niveles que puede tener el árbol.\n",
    "\n",
    "\n",
    "- **min_samples_leaf:** el número mínimo de muestras requeridas para que un nodo sea considerado una hoja (un nodo terminal). Si el número de muestras en un nodo es menor que este valor, no se permitirá la división adicional.\n",
    "\n",
    "\n",
    "- **min_samples_split:** el número mínimo de muestras requeridas para que un nodo pueda ser dividido en dos hijos. Si el número de muestras en un nodo es menor que este valor, la división no se realizará.\n",
    "\n",
    "    \"*min_samples_leaf* se refiere al número mínimo de muestras que deben estar en un nodo hoja del árbol. Un nodo hoja es aquel que no tiene ramificaciones adicionales, es decir, es un nodo terminal. Si después de una división, el número de muestras en un nodo es menor que el valor de min_samples_leaf, entonces la división no se realiza y el nodo se convierte en una hoja. Es decir, si min_samples_leaf está establecido en 1, entonces el árbol se dividirá hasta que todas las hojas tengan una sola muestra.\n",
    "\n",
    "    Por ejemplo, si en una base de datos de flores, se establece min_samples_leaf en 5, significa que cada hoja del árbol debe tener al menos 5 muestras. Si durante la construcción del árbol, en un nodo se encuentra que solo hay 3 muestras, la división no se realizará y el nodo se convertirá en una hoja.\n",
    "\n",
    "    *min_samples_split*, por otro lado, se refiere al número mínimo de muestras que se requieren para que un nodo pueda dividirse. Es decir, si el número de muestras en un nodo es menor que min_samples_split, entonces no se puede realizar una división y ese nodo se convierte en una hoja.\n",
    "\n",
    "    Por ejemplo, si en una base de datos de coches, se establece min_samples_split en 10, significa que se necesitan al menos 10 muestras para que un nodo se pueda dividir en dos. Si en un nodo hay solo 8 muestras, la división no se realizará y ese nodo se convertirá en una hoja.\n",
    "\n",
    "En resumen, **min_samples_leaf y min_samples_split** son parámetros que controlan la complejidad del modelo de árbol de decisión y **previenen el sobreajuste**. Al aumentar estos valores, se reduce la profundidad del árbol y se limita la capacidad del modelo para memorizar los datos de entrenamiento. Por otro lado, si estos valores son demasiado grandes, el modelo podría ser demasiado simple y tener un rendimiento inferior en los datos de prueba.\n",
    "\n",
    "- **ccp_alpha:** parámetro de complejidad basada en costo (Cost-Complexity Pruning) utilizado para podar el árbol. Un valor mayor de ccp_alpha aumenta la cantidad de poda en el árbol, lo que puede ayudar a prevenir el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dde37",
   "metadata": {},
   "source": [
    "## Modelos NO-Supervisados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5376874",
   "metadata": {},
   "source": [
    "## Modelos Por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cfb87",
   "metadata": {},
   "source": [
    "# **Evaluación del Modelo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96654953",
   "metadata": {},
   "source": [
    "Las medidas de evaluación de un modelo son una forma de medir el rendimiento o la capacidad predictiva de un modelo de aprendizaje automático. Estas medidas proporcionan información sobre qué tan bien el modelo puede generalizar a nuevos datos y predecir resultados precisos.\n",
    "\n",
    "<img src=https://miro.medium.com/v2/resize:fit:640/format:webp/1*tBErXYVvTw2jSUYK7thU2A.png width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a73865",
   "metadata": {},
   "source": [
    "## Evaluación de Regresión (errores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e1f99",
   "metadata": {},
   "source": [
    "- **El Error Cuadrático Medio (MSE, por sus siglas en inglés):** mide el promedio de los errores al cuadrado entre las predicciones del modelo y los valores reales. Es útil para modelos en los que se quiere penalizar más los errores grandes. Sin embargo, al elevar al cuadrado los errores, el MSE puede verse afectado por valores atípicos en los datos.\n",
    "\n",
    "                                        r2_score(y_train, y_pred)\n",
    "\n",
    "\n",
    "- **La Raíz del Error Cuadrático Medio (RMSE, por sus siglas en inglés):** es la raíz cuadrada del MSE. Se utiliza para obtener una métrica que tenga las mismas unidades que la variable objetivo, lo que facilita la interpretación de los resultados. Al igual que el MSE, el RMSE puede verse afectado por valores atípicos.\n",
    "\n",
    "                                np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "\n",
    "- **El Error Absoluto Medio (MAE, por sus siglas en inglés):** mide el promedio de las diferencias absolutas entre las predicciones del modelo y los valores reales. Es útil para modelos en los que se quiere penalizar por igual los errores grandes y pequeños. Al no elevar al cuadrado los errores, el MAE no se ve afectado por valores atípicos en los datos.\n",
    "\n",
    "                                    mean_squared_error(y_train, y_pred)\n",
    "\n",
    "En resumen, la elección de la métrica dependerá del tipo de problema y del contexto en el que se está trabajando. En algunos casos, puede ser más apropiado utilizar el MSE o el RMSE, mientras que en otros casos el MAE puede ser una mejor opción. En general, es recomendable utilizar varias métricas para evaluar el modelo y obtener una visión más completa de su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46addf2a",
   "metadata": {},
   "source": [
    "## Evalución de Logistica (Clasificación)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7339dff",
   "metadata": {},
   "source": [
    "**Comunes**\n",
    "- **Exactitud (Accuracy):** mide la proporción de predicciones correctas del modelo, es decir, la cantidad de observaciones clasificadas correctamente en comparación con el número total de observaciones.\n",
    "\n",
    "\n",
    "- **Precisión (Precision):** mide la proporción de verdaderos positivos (TP) entre todas las predicciones positivas, es decir, la capacidad del modelo de no clasificar incorrectamente una observación negativa como positiva.\n",
    "\n",
    "\n",
    "- **Sensibilidad (Recall):** mide la proporción de verdaderos positivos (TP) entre todas las observaciones positivas, es decir, la capacidad del modelo de identificar correctamente todas las observaciones positivas.\n",
    "\n",
    "\n",
    "- **F1 score:** es una medida que combina la precisión y la sensibilidad para proporcionar una medida única del rendimiento del modelo.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/800px-Precisionrecall.svg.png width=\"300\">\n",
    "\n",
    "- **AUC-ROC:** mide el área bajo la curva de la curva ROC (Receiver Operating Characteristic) y proporciona una medida de la capacidad del modelo para discriminar entre las clases positiva y negativa.\n",
    "\n",
    "\n",
    "<img src=https://www.themachinelearners.com/wp-content/uploads/2020/12/roc-curve-v2.png width=\"500\">\n",
    "\n",
    "\n",
    "~~~~\n",
    "logit_roc_auc = roc_auc_score(y_test, logisticRegr.predict(X_test_final))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logisticRegr.predict_proba(X_test_final)[:,1])\n",
    "~~~~\n",
    "\n",
    "**Otros**\n",
    "- **AUC-PR (área bajo la curva precision-recall):** es una medida de evaluación utilizada en problemas de clasificación binaria para evaluar la calidad del modelo al predecir la clase positiva en lugar de la negativa. Es especialmente útil en casos donde la clase positiva es rara.\n",
    "\n",
    "\n",
    "- **Log-loss:** es una medida de evaluación que se utiliza en problemas de clasificación para evaluar la capacidad del modelo para predecir la probabilidad de que una instancia pertenezca a una clase determinada.\n",
    "\n",
    "\n",
    "- **Brier score:** es una medida de evaluación que mide la diferencia entre las probabilidades predichas y las observadas. Se utiliza para evaluar la calidad de las predicciones de probabilidades en problemas de clasificación.\n",
    "\n",
    "\n",
    "- **Cohen's kappa:** es una medida de evaluación utilizada en problemas de clasificación para medir la concordancia entre dos clasificadores. Es especialmente útil cuando las clases no están equilibradas.\n",
    "\n",
    "\n",
    "- **MCC (Matthews Correlation Coefficient):** es una medida de evaluación utilizada en problemas de clasificación binaria para medir la calidad de la clasificación. MCC toma en cuenta los cuatro valores de la tabla de confusión y se considera una medida más equilibrada que la exactitud en problemas con clases desequilibradas.\n",
    "\n",
    "\n",
    "- **Gini coefficient:** es una medida de evaluación que mide la diferencia entre la curva de Lorenz y la línea de igualdad. Se utiliza en problemas de clasificación binaria para evaluar la calidad del modelo.\n",
    "\n",
    "\n",
    "- **KS (Kolmogorov-Smirnov) statistic:** es una medida de evaluación utilizada en problemas de clasificación binaria para medir la calidad de la clasificación. Mide la distancia máxima entre las funciones de distribución acumulativa de la clase positiva y la clase negativa.\n",
    "\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide la diferencia entre los valores observados y los valores predichos por el modelo.\n",
    "\n",
    "\n",
    "- **MAE (Mean Absolute Error):** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide la diferencia absoluta entre los valores observados y los valores predichos por el modelo.\n",
    "\n",
    "\n",
    "- **R-squared (coeficiente de determinación):** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide la proporción de la varianza en la variable dependiente que se explica por la variable independiente.\n",
    "\n",
    "\n",
    "- **Explained variance score:** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide la proporción de la varianza en la variable dependiente que se explica por el modelo.\n",
    "\n",
    "\n",
    "- **Mean squared logarithmic error:** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide la diferencia entre los valores observados y los valores predichos por el modelo utilizando una escala logarítmica.\n",
    "\n",
    "\n",
    "- **Mean Poisson deviance:** es una medida de evaluación utilizada en problemas de regresión de Poisson para evaluar la calidad del modelo. Mide la diferencia entre los valores observados y los valores predichos por el modelo utilizando la distribución de Poisson.\n",
    "\n",
    "\n",
    "- **Mean gamma deviance:** es una medida de evaluación utilizada en problemas de regresión gamma para evaluar la calidad del modelo. Mide la diferencia entre los valores observados y los valores predichos por el modelo utilizando la distribución gamma.\n",
    "\n",
    "\n",
    "- **Mean absolute percentage error (MAPE):** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide el porcentaje promedio de la diferencia absoluta entre los valores observados y los valores predichos por el modelo en relación con los valores observados.\n",
    "\n",
    "\n",
    "- **Symmetric mean absolute percentage error (SMAPE):** es una medida de evaluación utilizada en problemas de regresión para evaluar la calidad del modelo. Mide el porcentaje promedio de la diferencia absoluta simétrica entre los valores observados y los valores predichos por el modelo en relación con los valores observados.\n",
    "\n",
    "\n",
    "- **Mean absolute scaled error (MASE):** es una medida de evaluación utilizada en problemas de series de tiempo para evaluar la calidad del modelo. Mide la capacidad del modelo para predecir valores futuros en relación con la precisión de una predicción ingenua.\n",
    "\n",
    "\n",
    "- **Mean directional accuracy (MDA):** es una medida de evaluación utilizada en problemas de clasificación multiclase para evaluar la calidad del modelo. Mide la capacidad del modelo para predecir la dirección correcta de un cambio en la variable de respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a78cee",
   "metadata": {},
   "source": [
    "### RESIDUOS\n",
    "En el contexto de un modelo estadístico, los residuos son la diferencia entre los valores observados y los valores predichos por el modelo. Matemáticamente, si y es el valor observado y f(x) es el valor predicho por el modelo para una observación con un valor de entrada x, el residuo (r) para esa observación se calcula como:\n",
    "\n",
    "<img src=https://www.maximaformacion.es/wp-content/uploads/2021/07/que-son-los-residuos-en-un-modelo-de-regresion.jpg width=\"400\">\n",
    "\n",
    "~~~~\n",
    "                                            r = y - f(x)\n",
    "\n",
    "                                  - Calculamos los residuos del modelo\n",
    "                                    residuos_modelo = y_train - y_modelo\n",
    "\n",
    "                                  - Realizamos el gráfico\n",
    "                                    plt.figure(figsize=(10,7))\n",
    "                                    plt.scatter(x=y_modelo, y=residuos_modelo, alpha=0.6, c='royalblue', edgecolor='black')\n",
    "                                    plt.axhline(y=0, c='black', ls='--', linewidth=2.5)\n",
    "                                    plt.title(\"Modelo Superficie y Baños\");\n",
    "\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c6940",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6bae0f9",
   "metadata": {},
   "source": [
    "# Selección del Mejor Modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ca658",
   "metadata": {},
   "source": [
    "- **R-cuadrado ajustado (Adjusted R-squared):** Este criterio mide el ajuste del modelo en términos de la cantidad de variabilidad de la variable objetivo explicada por el modelo en relación a la variabilidad total de la variable objetivo, ajustando por el número de variables en el modelo. Valores más altos de R-cuadrado ajustado indican un mejor ajuste del modelo.<br><br>\n",
    "- **Criterio de información de Akaike (Akaike Information Criterion, AIC):** Este criterio mide la calidad del modelo en términos de la capacidad predictiva y la complejidad del modelo, ajustando por el número de observaciones en el conjunto de datos. Valores más bajos de AIC indican un mejor ajuste del modelo.<br><br>\n",
    "- **Criterio de información bayesiano (Bayesian Information Criterion, BIC)**: Este criterio es similar al AIC, pero penaliza más fuertemente los modelos más complejos. Valores más bajos de BIC indican un mejor ajuste del modelo.<br><br>\n",
    "- **Error cuadrático medio (Mean Squared Error, MSE):** Este criterio mide la calidad del modelo en términos de la precisión de las predicciones del modelo, en comparación con los valores reales. Valores más bajos de MSE indican un mejor ajuste del modelo.<br><br>\n",
    "- **Validación cruzada (Cross-validation):** Este criterio implica dividir el conjunto de datos en subconjuntos de entrenamiento y prueba, ajustando el modelo en el subconjunto de entrenamiento y evaluando la precisión de las predicciones en el subconjunto de prueba. El mejor modelo es aquel que tiene el menor error de predicción en el conjunto de prueba.<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "485.575px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
